<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Demo :: DLT-META</title><link>https://databrickslabs.github.io/dlt-meta/demo/index.html</link><description>DAIS 2023 DEMO: Showcases DLT-META’s capabilities of creating Bronze and Silver Lakeflow Declarative Pipelines with initial and incremental mode automatically. Databricks Techsummit Demo: 100s of data sources ingestion in bronze and silver Lakeflow Declarative Pipelines automatically. Append FLOW Autoloader Demo: Write to same target from multiple sources using append_flow and adding file metadata using File metadata column Append FLOW Eventhub Demo: Write to same target from multiple sources using append_flow and adding using File metadata column Silver Fanout Demo: This demo will showcase fanout architecture can be implemented in silver layer Apply Changes From Snapshot Demo: This demo will showcase create_auto_cdc_from_snapshot_flow can be implemented inside bronze and silver layer</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 04 Oct 2024 14:25:26 -0400</lastBuildDate><atom:link href="https://databrickslabs.github.io/dlt-meta/demo/index.xml" rel="self" type="application/rss+xml"/><item><title>DAIS DEMO</title><link>https://databrickslabs.github.io/dlt-meta/demo/dais/index.html</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/dais/index.html</guid><description>DAIS 2023 DEMO: DAIS 2023 Session Recording This demo showcases DLT-META’s capabilities of creating Bronze and Silver DLT pipelines with initial and incremental mode automatically.
Customer and Transactions feeds for initial load Adds new feeds Product and Stores to existing Bronze and Silver DLT pipelines with metadata changes. Runs Bronze and Silver DLT for incremental load for CDC events Steps to launch DAIS demo in your Databricks workspace: Launch Command Prompt</description></item><item><title>Tech Summit DEMO</title><link>https://databrickslabs.github.io/dlt-meta/demo/techsummit/index.html</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/techsummit/index.html</guid><description>Databricks Tech Summit FY2024 DEMO: This demo will launch auto generated tables(100s) inside single bronze and silver DLT pipeline using dlt-meta.
Launch Command Prompt
Install Databricks CLI
Once you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal
dlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run the command python demo/launch_techsummit_demo.py --uc_catalog_name=&lt;&lt;Unity Catalog name>> --cloud_provider_name=aws</description></item><item><title>Append FLOW Autoloader Demo</title><link>https://databrickslabs.github.io/dlt-meta/demo/append_flow_cf/index.html</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/append_flow_cf/index.html</guid><description>Append FLOW Autoloader Demo: This demo will perform following tasks:
Read from different source paths using autoloader and write to same target using dlt.append_flow API Read from different delta tables and write to same silver table using append_flow API Add file_name and file_path to target bronze table for autoloader source using File metadata column Append flow with autoloader Launch Command Prompt
Install Databricks CLI
Once you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal</description></item><item><title>Append FLOW Eventhub Demo</title><link>https://databrickslabs.github.io/dlt-meta/demo/append_flow_eh/index.html</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/append_flow_eh/index.html</guid><description>Append FLOW Autoloader Demo: Read from different eventhub topics and write to same target tables using dlt.append_flow API Steps: Launch Command Prompt
Install Databricks CLI
Once you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal
dlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Eventhub
Needs eventhub instance running
Need two eventhub topics first for main feed (eventhub_name) and second for append flow feed (eventhub_name_append_flow)</description></item><item><title>Silver Fanout Demo</title><link>https://databrickslabs.github.io/dlt-meta/demo/silver_fanout/index.html</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/silver_fanout/index.html</guid><description>Silver Fanout Demo This demo will perform following steps Showcase onboarding process for silver fanout pattern Run onboarding for the bronze cars table, which contains data from various countries. Run onboarding for the silver tables, which have a where_clause based on the country condition in silver_transformations_cars.json. Run Bronze for cars tables Run onboarding for the silver tables, fanning out from the bronze cars tables to country-specific tables such as cars_usa, cars_uk, cars_germany, and cars_japan. Steps: Launch Command Prompt</description></item><item><title>Silver Fanout Demo</title><link>https://databrickslabs.github.io/dlt-meta/demo/apply_changes_from_snapshot/index.html</link><pubDate>Fri, 04 Oct 2024 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/demo/apply_changes_from_snapshot/index.html</guid><description>Apply Changes From Snapshot Demo This demo will perform following steps Showcase onboarding process for apply changes from snapshot pattern Run onboarding for the bronze stores and products tables, which contains data snapshot data in csv files. Run Bronze DLT to load initial snapshot (LOAD_1.csv) Upload incremental snapshot LOAD_2.csv version=2 for stores and product Run Bronze DLT to load incremental snapshot (LOAD_2.csv). Stores is scd_type=2 so updated records will expired and added new records with version_number. Products is scd_type=1 so in case records missing for scd_type=1 will be deleted. Upload incremental snapshot LOAD_3.csv version=3 for stores and product Run Bronze DLT to load incremental snapshot (LOAD_3.csv). Stores is scd_type=2 so updated records will expired and added new records with version_number. Products is scd_type=1 so in case records missing for scd_type=1 will be deleted. Steps: Launch Command Prompt</description></item></channel></rss>