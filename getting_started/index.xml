<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Getting Started :: DLT-META</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/index.html</link><description>The following tutorial will guide you through the process for setting up the DLT-META on your Databricks Lakehouse environment.
You will deploy/configure the solution, configure a database/table for bronze and silver layer as per below stages.</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 31 Aug 2025 14:25:26 -0400</lastBuildDate><atom:link href="https://databrickslabs.github.io/dlt-meta/getting_started/index.xml" rel="self" type="application/rss+xml"/><item><title>Metadata Preparation</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/index.html</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/index.html</guid><description>Directory structure conf/ onboarding.json silver_transformations.json dqe/ bronze_data_quality_expectations.json Create onboarding.json Create silver_transformations.json Create data quality rules json’s for each entity e.g. Data Quality Rules The onboarding.json file contains links to silver_transformations.json and data quality expectation files dqe.
onboarding.json File structure: Examples( Autoloader, Eventhub, Kafka ) env is your environment placeholder e.g dev, prod, stag
Field Description data_flow_id This is unique identifier for pipeline data_flow_group This is group identifier for launching multiple pipelines under single DLT source_format Source format e.g cloudFiles, eventhub, kafka, delta, snapshot source_details This map Type captures all source details for cloudfiles = source_schema_path, source_path_{env}, source_catalog, source_database, source_metadata For eventhub= source_schema_path , eventhub.accessKeyName, eventhub.accessKeySecretName, eventhub.name , eventhub.secretsScopeName , kafka.sasl.mechanism, kafka.security.protocol, eventhub.namespace, eventhub.port. For Source schema file spark DDL schema format parsing is supported In case of custom schema format then write schema parsing function bronze_schema_mapper(schema_file_path, spark):Schema and provide to OnboardDataflowspec initialization e.g onboardDataFlowSpecs = OnboardDataflowspec(spark, dict_obj,bronze_schema_mapper).onboardDataFlowSpecs(). For cloudFiles option _metadata columns addtiion there is source_metadata tag with attributes: include_autoloader_metadata_column flag (True or False value) will add _metadata column to target bronze dataframe, autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata,select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column. for snapshot= snapshot_format, source_path_{env} bronze_catalog_{env} Unity catalog name bronze_database_{env} Delta lake bronze database name. bronze_table Delta lake bronze table name bronze_table_comment Bronze table comment bronze_reader_options Reader options which can be provided to spark reader e.g multiline=true,header=true in json format bronze_parition_columns Bronze table partition cols list bronze_cluster_by Bronze tables cluster by cols list bronze_cdc_apply_changes Bronze cdc apply changes Json bronze_apply_changes_from_snapshot Bronze apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] bronze_table_path_{env} Bronze table storage path. bronze_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" } bronze_sink DLT Sink API properties: e.g Delta: {"name": "bronze_sink","format": "delta","options": {"tableName": "my_catalog.my_schema.my_table"}}, Kafka:{"name": "bronze_sink","format": "kafka","options": { "kafka.bootstrap.servers": "host:port","subscribe": "my_topic"}} bronze_data_quality_expectations_json Bronze table data quality expectations bronze_catalog_quarantine_{env} Unity catalog name bronze_database_quarantine_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table Bronze Table for quarantine data which fails expectations bronze_quarantine_table_comment Bronze quarantine table comment bronze_quarantine_table_path_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table_partitions Bronze quarantine tables partition cols bronze_quarantine_table_cluster_by Bronze quarantine tables cluster cols bronze_quarantine_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" } bronze_append_flows Bronze table append flows json. e.g."bronze_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}] silver_catalog_{env} Unit Catalog name. silver_database_{env} Silver database name. silver_table Silver table name silver_table_comment Silver table comments silver_partition_columns Silver table partition columns list silver_cluster_by Silver tables cluster by cols list silver_cdc_apply_changes Silver cdc apply changes Json silver_table_path_{env} Silver table storage path. silver_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false"} silver_sink DLT Sink API properties: e.g Delta:{"name": "silver_sink","format": "delta","options": {"tableName": "my_catalog.my_schema.my_table"}}, Kafka:{"name": "silver_sink","format": "kafka","options": { "kafka.bootstrap.servers": "host:port","subscribe": "my_topic"}} silver_transformation_json Silver table sql transformation json path silver_data_quality_expectations_json_{env} Silver table data quality expectations json file path silver_append_flows Silver table append flows json. e.g.`“silver_append_flows”:[{“name”:“customer_bronze_flow”, silver_apply_changes_from_snapshot Silver apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] Data Quality Rules File Structure(Examples) Field Description expect Specify multiple data quality sql for each field when records that fail validation should be included in the target dataset expect_or_fail Specify multiple data quality sql for each field when records that fail validation should halt pipeline execution expect_or_drop Specify multiple data quality sql for each field when records that fail validation should be dropped from the target dataset expect_or_quarantine Specify multiple data quality sql for each field when records that fails validation will be dropped from main table and inserted into quarantine table specified in dataflowspec (only applicable for Bronze layer) Silver transformation File Structure(Example) Field Description target_table Specify target table name : Type String target_partition_cols Specify partition columns : Type Array select_exp Specify SQL expressions : Type Array where_clause Specify filter conditions if you want to prevent certain records from main input : Type Array</description></item><item><title>DLT-META CLI</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_cli/index.html</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_cli/index.html</guid><description>Prerequisites: Python 3.8.0 + Databricks CLI Steps: Install and authenticate Databricks CLI:
databricks auth login --host WORKSPACE_HOST Install dlt-meta via Databricks CLI:
databricks labs install dlt-meta Clone dlt-meta repository:
git clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:
cd dlt-meta Create Python virtual environment:
python -m venv .venv Activate virtual environment:
source .venv/bin/activate Install required packages:
# Core requirements pip install "PyYAML>=6.0" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest>=7.0.0 coverage>=7.0.0 pyspark==3.5.5 # Integration test requirements pip install "typer[all]==0.6.1" Set environment variables:</description></item><item><title>DLT-META Manual</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_manual/index.html</link><pubDate>Wed, 04 Aug 2021 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_manual/index.html</guid><description>OnboardJob Option#1: using Databricks Python whl job Go to your Databricks landing page and do one of the following:
In the sidebar, click Jobs Icon Workflows and click Create Job Button.
In the sidebar, click New Icon New and select Job from the menu.
In the task dialog box that appears on the Tasks tab, replace Add a name for your job… with your job name, for example, Python wheel example.</description></item><item><title>DLT-META Lakehouse App</title><link>https://databrickslabs.github.io/dlt-meta/getting_started/app/index.html</link><pubDate>Sun, 31 Aug 2025 14:25:26 -0400</pubDate><guid>https://databrickslabs.github.io/dlt-meta/getting_started/app/index.html</guid><description>Prerequisites System Requirements Python 3.8.0 or higher Databricks CLI (latest version, e.g., 0.244.0) Configured workspace access Initial Setup Authenticate with Databricks:
databricks auth login --host WORKSPACE_HOST Setup Python Environment:
git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk Deployment Options Deploy to Databricks Create Custom App:
databricks apps create demo-dltmeta Note: Wait for command completion (a few minutes)
Setup App Code:</description></item></channel></rss>