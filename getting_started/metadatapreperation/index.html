<!doctype html><html lang=en-us dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=html><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.143.0"><meta name=generator content="Relearn 7.2.1"><meta name=description content='Directory structure conf/ onboarding.json silver_transformations.json dqe/ bronze_data_quality_expectations.json Create onboarding.json Create silver_transformations.json Create data quality rules json’s for each entity e.g. Data Quality Rules The onboarding.json file contains links to silver_transformations.json and data quality expectation files dqe.
onboarding.json File structure: Examples( Autoloader, Eventhub, Kafka ) env is your environment placeholder e.g dev, prod, stag
Field Description data_flow_id This is unique identifier for pipeline data_flow_group This is group identifier for launching multiple pipelines under single DLT source_format Source format e.g cloudFiles, eventhub, kafka, delta, snapshot source_details This map Type captures all source details for cloudfiles = source_schema_path, source_path_{env}, source_database, source_metadata For eventhub= source_schema_path , eventhub.accessKeyName, eventhub.accessKeySecretName, eventhub.name , eventhub.secretsScopeName , kafka.sasl.mechanism, kafka.security.protocol, eventhub.namespace, eventhub.port. For Source schema file spark DDL schema format parsing is supported In case of custom schema format then write schema parsing function bronze_schema_mapper(schema_file_path, spark):Schema and provide to OnboardDataflowspec initialization e.g onboardDataFlowSpecs = OnboardDataflowspec(spark, dict_obj,bronze_schema_mapper).onboardDataFlowSpecs(). For cloudFiles option _metadata columns addtiion there is source_metadata tag with attributes: include_autoloader_metadata_column flag (True or False value) will add _metadata column to target bronze dataframe, autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata,select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column. for snapshot= snapshot_format, source_path_{env} bronze_database_{env} Delta lake bronze database name. bronze_table Delta lake bronze table name bronze_reader_options Reader options which can be provided to spark reader e.g multiline=true,header=true in json format bronze_parition_columns Bronze table partition cols list bronze_cluster_by Bronze tables cluster by cols list bronze_cdc_apply_changes Bronze cdc apply changes Json bronze_apply_changes_from_snapshot Bronze apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] bronze_table_path_{env} Bronze table storage path. bronze_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" } bronze_data_quality_expectations_json Bronze table data quality expectations bronze_database_quarantine_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table	Bronze Table for quarantine data which fails expectations bronze_quarantine_table_path_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table_partitions Bronze quarantine tables partition cols bronze_quarantine_table_cluster_by Bronze quarantine tables cluster cols bronze_quarantine_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" } bronze_append_flows Bronze table append flows json. e.g."bronze_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}] silver_database_{env} Silver database name. silver_table Silver table name silver_partition_columns Silver table partition columns list silver_cluster_by Silver tables cluster by cols list silver_cdc_apply_changes Silver cdc apply changes Json silver_table_path_{env} Silver table storage path. silver_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false"} silver_transformation_json Silver table sql transformation json path silver_data_quality_expectations_json_{env} Silver table data quality expectations json file path silver_append_flows Silver table append flows json. e.g."silver_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}] Data Quality Rules File Structure(Examples) Field Description expect Specify multiple data quality sql for each field when records that fail validation should be included in the target dataset expect_or_fail Specify multiple data quality sql for each field when records that fail validation should halt pipeline execution expect_or_drop Specify multiple data quality sql for each field when records that fail validation should be dropped from the target dataset expect_or_quarantine Specify multiple data quality sql for each field when records that fails validation will be dropped from main table and inserted into quarantine table specified in dataflowspec (only applicable for Bronze layer) Silver transformation File Structure(Example) Field Description target_table Specify target table name : Type String target_partition_cols Specify partition columns : Type Array select_exp Specify SQL expressions : Type Array where_clause Specify filter conditions if you want to prevent certain records from main input : Type Array'><meta name=author content="Ravi Gawai (Databricks)"><meta name=twitter:card content="summary"><meta name=twitter:title content="Metadata Preparation :: DLT-META"><meta name=twitter:description content='Directory structure conf/ onboarding.json silver_transformations.json dqe/ bronze_data_quality_expectations.json Create onboarding.json Create silver_transformations.json Create data quality rules json’s for each entity e.g. Data Quality Rules The onboarding.json file contains links to silver_transformations.json and data quality expectation files dqe.
onboarding.json File structure: Examples( Autoloader, Eventhub, Kafka ) env is your environment placeholder e.g dev, prod, stag
Field Description data_flow_id This is unique identifier for pipeline data_flow_group This is group identifier for launching multiple pipelines under single DLT source_format Source format e.g cloudFiles, eventhub, kafka, delta, snapshot source_details This map Type captures all source details for cloudfiles = source_schema_path, source_path_{env}, source_database, source_metadata For eventhub= source_schema_path , eventhub.accessKeyName, eventhub.accessKeySecretName, eventhub.name , eventhub.secretsScopeName , kafka.sasl.mechanism, kafka.security.protocol, eventhub.namespace, eventhub.port. For Source schema file spark DDL schema format parsing is supported In case of custom schema format then write schema parsing function bronze_schema_mapper(schema_file_path, spark):Schema and provide to OnboardDataflowspec initialization e.g onboardDataFlowSpecs = OnboardDataflowspec(spark, dict_obj,bronze_schema_mapper).onboardDataFlowSpecs(). For cloudFiles option _metadata columns addtiion there is source_metadata tag with attributes: include_autoloader_metadata_column flag (True or False value) will add _metadata column to target bronze dataframe, autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata,select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column. for snapshot= snapshot_format, source_path_{env} bronze_database_{env} Delta lake bronze database name. bronze_table Delta lake bronze table name bronze_reader_options Reader options which can be provided to spark reader e.g multiline=true,header=true in json format bronze_parition_columns Bronze table partition cols list bronze_cluster_by Bronze tables cluster by cols list bronze_cdc_apply_changes Bronze cdc apply changes Json bronze_apply_changes_from_snapshot Bronze apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] bronze_table_path_{env} Bronze table storage path. bronze_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" } bronze_data_quality_expectations_json Bronze table data quality expectations bronze_database_quarantine_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table	Bronze Table for quarantine data which fails expectations bronze_quarantine_table_path_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table_partitions Bronze quarantine tables partition cols bronze_quarantine_table_cluster_by Bronze quarantine tables cluster cols bronze_quarantine_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" } bronze_append_flows Bronze table append flows json. e.g."bronze_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}] silver_database_{env} Silver database name. silver_table Silver table name silver_partition_columns Silver table partition columns list silver_cluster_by Silver tables cluster by cols list silver_cdc_apply_changes Silver cdc apply changes Json silver_table_path_{env} Silver table storage path. silver_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false"} silver_transformation_json Silver table sql transformation json path silver_data_quality_expectations_json_{env} Silver table data quality expectations json file path silver_append_flows Silver table append flows json. e.g."silver_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}] Data Quality Rules File Structure(Examples) Field Description expect Specify multiple data quality sql for each field when records that fail validation should be included in the target dataset expect_or_fail Specify multiple data quality sql for each field when records that fail validation should halt pipeline execution expect_or_drop Specify multiple data quality sql for each field when records that fail validation should be dropped from the target dataset expect_or_quarantine Specify multiple data quality sql for each field when records that fails validation will be dropped from main table and inserted into quarantine table specified in dataflowspec (only applicable for Bronze layer) Silver transformation File Structure(Example) Field Description target_table Specify target table name : Type String target_partition_cols Specify partition columns : Type Array select_exp Specify SQL expressions : Type Array where_clause Specify filter conditions if you want to prevent certain records from main input : Type Array'><meta property="og:url" content="https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/index.html"><meta property="og:site_name" content="DLT-META"><meta property="og:title" content="Metadata Preparation :: DLT-META"><meta property="og:description" content='Directory structure conf/ onboarding.json silver_transformations.json dqe/ bronze_data_quality_expectations.json Create onboarding.json Create silver_transformations.json Create data quality rules json’s for each entity e.g. Data Quality Rules The onboarding.json file contains links to silver_transformations.json and data quality expectation files dqe.
onboarding.json File structure: Examples( Autoloader, Eventhub, Kafka ) env is your environment placeholder e.g dev, prod, stag
Field Description data_flow_id This is unique identifier for pipeline data_flow_group This is group identifier for launching multiple pipelines under single DLT source_format Source format e.g cloudFiles, eventhub, kafka, delta, snapshot source_details This map Type captures all source details for cloudfiles = source_schema_path, source_path_{env}, source_database, source_metadata For eventhub= source_schema_path , eventhub.accessKeyName, eventhub.accessKeySecretName, eventhub.name , eventhub.secretsScopeName , kafka.sasl.mechanism, kafka.security.protocol, eventhub.namespace, eventhub.port. For Source schema file spark DDL schema format parsing is supported In case of custom schema format then write schema parsing function bronze_schema_mapper(schema_file_path, spark):Schema and provide to OnboardDataflowspec initialization e.g onboardDataFlowSpecs = OnboardDataflowspec(spark, dict_obj,bronze_schema_mapper).onboardDataFlowSpecs(). For cloudFiles option _metadata columns addtiion there is source_metadata tag with attributes: include_autoloader_metadata_column flag (True or False value) will add _metadata column to target bronze dataframe, autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata,select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column. for snapshot= snapshot_format, source_path_{env} bronze_database_{env} Delta lake bronze database name. bronze_table Delta lake bronze table name bronze_reader_options Reader options which can be provided to spark reader e.g multiline=true,header=true in json format bronze_parition_columns Bronze table partition cols list bronze_cluster_by Bronze tables cluster by cols list bronze_cdc_apply_changes Bronze cdc apply changes Json bronze_apply_changes_from_snapshot Bronze apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] bronze_table_path_{env} Bronze table storage path. bronze_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" } bronze_data_quality_expectations_json Bronze table data quality expectations bronze_database_quarantine_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table	Bronze Table for quarantine data which fails expectations bronze_quarantine_table_path_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table_partitions Bronze quarantine tables partition cols bronze_quarantine_table_cluster_by Bronze quarantine tables cluster cols bronze_quarantine_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" } bronze_append_flows Bronze table append flows json. e.g."bronze_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}] silver_database_{env} Silver database name. silver_table Silver table name silver_partition_columns Silver table partition columns list silver_cluster_by Silver tables cluster by cols list silver_cdc_apply_changes Silver cdc apply changes Json silver_table_path_{env} Silver table storage path. silver_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false"} silver_transformation_json Silver table sql transformation json path silver_data_quality_expectations_json_{env} Silver table data quality expectations json file path silver_append_flows Silver table append flows json. e.g."silver_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}] Data Quality Rules File Structure(Examples) Field Description expect Specify multiple data quality sql for each field when records that fail validation should be included in the target dataset expect_or_fail Specify multiple data quality sql for each field when records that fail validation should halt pipeline execution expect_or_drop Specify multiple data quality sql for each field when records that fail validation should be dropped from the target dataset expect_or_quarantine Specify multiple data quality sql for each field when records that fails validation will be dropped from main table and inserted into quarantine table specified in dataflowspec (only applicable for Bronze layer) Silver transformation File Structure(Example) Field Description target_table Specify target table name : Type String target_partition_cols Specify partition columns : Type Array select_exp Specify SQL expressions : Type Array where_clause Specify filter conditions if you want to prevent certain records from main input : Type Array'><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="Getting Started"><meta property="article:published_time" content="2021-08-04T14:25:26-04:00"><meta property="article:modified_time" content="2021-08-04T14:25:26-04:00"><meta itemprop=name content="Metadata Preparation :: DLT-META"><meta itemprop=description content='Directory structure conf/ onboarding.json silver_transformations.json dqe/ bronze_data_quality_expectations.json Create onboarding.json Create silver_transformations.json Create data quality rules json’s for each entity e.g. Data Quality Rules The onboarding.json file contains links to silver_transformations.json and data quality expectation files dqe.
onboarding.json File structure: Examples( Autoloader, Eventhub, Kafka ) env is your environment placeholder e.g dev, prod, stag
Field Description data_flow_id This is unique identifier for pipeline data_flow_group This is group identifier for launching multiple pipelines under single DLT source_format Source format e.g cloudFiles, eventhub, kafka, delta, snapshot source_details This map Type captures all source details for cloudfiles = source_schema_path, source_path_{env}, source_database, source_metadata For eventhub= source_schema_path , eventhub.accessKeyName, eventhub.accessKeySecretName, eventhub.name , eventhub.secretsScopeName , kafka.sasl.mechanism, kafka.security.protocol, eventhub.namespace, eventhub.port. For Source schema file spark DDL schema format parsing is supported In case of custom schema format then write schema parsing function bronze_schema_mapper(schema_file_path, spark):Schema and provide to OnboardDataflowspec initialization e.g onboardDataFlowSpecs = OnboardDataflowspec(spark, dict_obj,bronze_schema_mapper).onboardDataFlowSpecs(). For cloudFiles option _metadata columns addtiion there is source_metadata tag with attributes: include_autoloader_metadata_column flag (True or False value) will add _metadata column to target bronze dataframe, autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata,select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column. for snapshot= snapshot_format, source_path_{env} bronze_database_{env} Delta lake bronze database name. bronze_table Delta lake bronze table name bronze_reader_options Reader options which can be provided to spark reader e.g multiline=true,header=true in json format bronze_parition_columns Bronze table partition cols list bronze_cluster_by Bronze tables cluster by cols list bronze_cdc_apply_changes Bronze cdc apply changes Json bronze_apply_changes_from_snapshot Bronze apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] bronze_table_path_{env} Bronze table storage path. bronze_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" } bronze_data_quality_expectations_json Bronze table data quality expectations bronze_database_quarantine_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table	Bronze Table for quarantine data which fails expectations bronze_quarantine_table_path_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table_partitions Bronze quarantine tables partition cols bronze_quarantine_table_cluster_by Bronze quarantine tables cluster cols bronze_quarantine_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" } bronze_append_flows Bronze table append flows json. e.g."bronze_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}] silver_database_{env} Silver database name. silver_table Silver table name silver_partition_columns Silver table partition columns list silver_cluster_by Silver tables cluster by cols list silver_cdc_apply_changes Silver cdc apply changes Json silver_table_path_{env} Silver table storage path. silver_table_properties DLT table properties map. e.g. {"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false"} silver_transformation_json Silver table sql transformation json path silver_data_quality_expectations_json_{env} Silver table data quality expectations json file path silver_append_flows Silver table append flows json. e.g."silver_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}] Data Quality Rules File Structure(Examples) Field Description expect Specify multiple data quality sql for each field when records that fail validation should be included in the target dataset expect_or_fail Specify multiple data quality sql for each field when records that fail validation should halt pipeline execution expect_or_drop Specify multiple data quality sql for each field when records that fail validation should be dropped from the target dataset expect_or_quarantine Specify multiple data quality sql for each field when records that fails validation will be dropped from main table and inserted into quarantine table specified in dataflowspec (only applicable for Bronze layer) Silver transformation File Structure(Example) Field Description target_table Specify target table name : Type String target_partition_cols Specify partition columns : Type Array select_exp Specify SQL expressions : Type Array where_clause Specify filter conditions if you want to prevent certain records from main input : Type Array'><meta itemprop=datePublished content="2021-08-04T14:25:26-04:00"><meta itemprop=dateModified content="2021-08-04T14:25:26-04:00"><meta itemprop=wordCount content="611"><title>Metadata Preparation :: DLT-META</title>
<link href=https://databrickslabs.github.io/dlt-meta/css/fontawesome-all.min.css?1739407516 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=https://databrickslabs.github.io/dlt-meta/css/fontawesome-all.min.css?1739407516 rel=stylesheet></noscript><link href=https://databrickslabs.github.io/dlt-meta/css/auto-complete.css?1739407516 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=https://databrickslabs.github.io/dlt-meta/css/auto-complete.css?1739407516 rel=stylesheet></noscript><link href=https://databrickslabs.github.io/dlt-meta/css/perfect-scrollbar.min.css?1739407516 rel=stylesheet><link href=https://databrickslabs.github.io/dlt-meta/css/theme.min.css?1739407516 rel=stylesheet><link href=https://databrickslabs.github.io/dlt-meta/css/format-html.min.css?1739407516 rel=stylesheet id=R-format-style><script>window.relearn=window.relearn||{},window.relearn.relBasePath="../..",window.relearn.relBaseUri="../..",window.relearn.absBaseUri="https://databrickslabs.github.io/dlt-meta",window.relearn.min=`.min`,window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.themevariants=["learn"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}}))},window.relearn.markVariant=function(){var t=window.localStorage.getItem(window.relearn.absBaseUri+"/variant"),e=document.querySelector("#R-select-variant");e&&(e.value=t)},window.relearn.initVariant=function(){var e=window.localStorage.getItem(window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.localStorage.setItem(window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant(),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script><script async defer src=https://buttons.github.io/buttons.js></script></head><body class="mobile-support html" data-url=https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div><div class="topbar-button topbar-button-toc" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title="Table of Contents (CTRL+ALT+t)"><i class="fa-fw fas fa-list-alt"></i></button><div class=topbar-content><div class=topbar-content-wrapper><nav class=TableOfContents><ul><li><ul><li><a href=#directory-structure>Directory structure</a></li><li><a href=#onboardingjson-file-structure-examples-autoloaderhttpsgithubcomdatabrickslabsdlt-metablobmainexamplescloudfiles-onboardingtemplate-eventhubhttpsgithubcomdatabrickslabsdlt-metablobmainexampleseventhub-onboardingtemplate-kafkahttpsgithubcomdatabrickslabsdlt-metablobmainexampleskafka-onboardingtemplate->onboarding.json File structure: Examples( <a href=https://github.com/databrickslabs/dlt-meta/blob/main/examples/cloudfiles-onboarding.template>Autoloader</a>, <a href=https://github.com/databrickslabs/dlt-meta/blob/main/examples/eventhub-onboarding.template>Eventhub</a>, <a href=https://github.com/databrickslabs/dlt-meta/blob/main/examples/kafka-onboarding.template>Kafka</a> )</a></li><li><a href=#data-quality-rules-file-structureexampleshttpsgithubcomdatabrickslabsdlt-metatreemainexamplesdqe>Data Quality Rules File Structure(<a href=https://github.com/databrickslabs/dlt-meta/tree/main/examples/dqe>Examples</a>)</a></li><li><a href=#silver-transformation-file-structureexamplehttpsgithubcomdatabrickslabsdlt-metablobmainexamplessilver_transformationsjson>Silver transformation File Structure(<a href=https://github.com/databrickslabs/dlt-meta/blob/main/examples/silver_transformations.json>Example</a>)</a></li></ul></li></ul></nav></div></div></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=https://databrickslabs.github.io/dlt-meta/index.html><span itemprop=name>DLT-META</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=https://databrickslabs.github.io/dlt-meta/getting_started/index.html><span itemprop=name>Getting Started</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Metadata Preparation</span><meta itemprop=position content="3"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-prev" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=https://databrickslabs.github.io/dlt-meta/getting_started/index.html title="Getting Started (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a></div><div class="topbar-button topbar-button-next" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_cli/index.html title="DLT-META CLI (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable getting_started" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline></header><h1 id=metadata-preparation>Metadata Preparation</h1><h3 id=directory-structure>Directory structure</h3><div class="highlight wrap-code"><pre tabindex=0><code>conf/
    onboarding.json
    silver_transformations.json
    dqe/
        bronze_data_quality_expectations.json</code></pre></div><ol><li>Create <a href=https://github.com/databrickslabs/dlt-meta/blob/main/demo/conf/onboarding.template rel=external target=_blank>onboarding.json</a></li><li>Create <a href=https://github.com/databrickslabs/dlt-meta/blob/main/demo/conf/silver_transformations.json rel=external target=_blank>silver_transformations.json</a></li><li>Create data quality rules json&rsquo;s for each entity e.g. <a href=https://github.com/databrickslabs/dlt-meta/tree/main/demo/conf/dqe/ rel=external target=_blank>Data Quality Rules</a></li></ol><p>The <code>onboarding.json</code> file contains links to <a href=https://github.com/databrickslabs/dlt-meta/blob/3555aaa798881a9cfa65f89599f83d22d245d3c8/demo/conf/onboarding.template#L41C1-L42C1 rel=external target=_blank>silver_transformations.json</a> and data quality expectation files <a href=https://github.com/databrickslabs/dlt-meta/blob/3555aaa798881a9cfa65f89599f83d22d245d3c8/demo/conf/onboarding.template#L42 rel=external target=_blank>dqe</a>.</p><h3 id=onboardingjson-file-structure-examples-autoloaderhttpsgithubcomdatabrickslabsdlt-metablobmainexamplescloudfiles-onboardingtemplate-eventhubhttpsgithubcomdatabrickslabsdlt-metablobmainexampleseventhub-onboardingtemplate-kafkahttpsgithubcomdatabrickslabsdlt-metablobmainexampleskafka-onboardingtemplate->onboarding.json File structure: Examples( <a href=https://github.com/databrickslabs/dlt-meta/blob/main/examples/cloudfiles-onboarding.template rel=external target=_blank>Autoloader</a>, <a href=https://github.com/databrickslabs/dlt-meta/blob/main/examples/eventhub-onboarding.template rel=external target=_blank>Eventhub</a>, <a href=https://github.com/databrickslabs/dlt-meta/blob/main/examples/kafka-onboarding.template rel=external target=_blank>Kafka</a> )</h3><p><code>env</code> is your environment placeholder e.g <code>dev</code>, <code>prod</code>, <code>stag</code></p><table><thead><tr><th style=text-align:center>Field</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:center>data_flow_id</td><td style=text-align:left>This is unique identifier for pipeline</td></tr><tr><td style=text-align:center>data_flow_group</td><td style=text-align:left>This is group identifier for launching multiple pipelines under single DLT</td></tr><tr><td style=text-align:center>source_format</td><td style=text-align:left>Source format e.g <code>cloudFiles</code>, <code>eventhub</code>, <code>kafka</code>, <code>delta</code>, <code>snapshot</code></td></tr><tr><td style=text-align:center>source_details</td><td style=text-align:left>This map Type captures all source details for cloudfiles = <code>source_schema_path</code>, <code>source_path_{env}</code>, <code>source_database</code>, <code>source_metadata</code> For eventhub= <code>source_schema_path</code> , <code>eventhub.accessKeyName</code>, <code>eventhub.accessKeySecretName</code>, <code>eventhub.name</code> , <code>eventhub.secretsScopeName</code> , <code>kafka.sasl.mechanism</code>, <code>kafka.security.protocol</code>, <code>eventhub.namespace</code>, <code>eventhub.port</code>. For Source schema file spark DDL schema format parsing is supported In case of custom schema format then write schema parsing function <code>bronze_schema_mapper(schema_file_path, spark):Schema</code> and provide to <code>OnboardDataflowspec</code> initialization e.g <code>onboardDataFlowSpecs = OnboardDataflowspec(spark, dict_obj,bronze_schema_mapper).onboardDataFlowSpecs()</code>. For cloudFiles option _metadata columns addtiion there is <code>source_metadata</code> tag with attributes: <code>include_autoloader_metadata_column</code> flag (<code>True</code> or <code>False</code> value) will add _metadata column to target bronze dataframe, <code>autoloader_metadata_col_name</code> if this provided then will be used to rename _metadata to this value otherwise default is <code>source_metadata</code>,<code>select_metadata_cols:{key:value}</code> will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column. for snapshot= <code>snapshot_format</code>, <code>source_path_{env}</code></td></tr><tr><td style=text-align:center>bronze_database_{env}</td><td style=text-align:left>Delta lake bronze database name.</td></tr><tr><td style=text-align:center>bronze_table</td><td style=text-align:left>Delta lake bronze table name</td></tr><tr><td style=text-align:center>bronze_reader_options</td><td style=text-align:left>Reader options which can be provided to spark reader e.g multiline=true,header=true in json format</td></tr><tr><td style=text-align:center>bronze_parition_columns</td><td style=text-align:left>Bronze table partition cols list</td></tr><tr><td style=text-align:center>bronze_cluster_by</td><td style=text-align:left>Bronze tables cluster by cols list</td></tr><tr><td style=text-align:center>bronze_cdc_apply_changes</td><td style=text-align:left>Bronze cdc apply changes Json</td></tr><tr><td style=text-align:center>bronze_apply_changes_from_snapshot</td><td style=text-align:left>Bronze apply changes from snapshot Json e.g. Mandatory fields: keys=[&ldquo;userId&rdquo;], scd_type=<code>1</code> or <code>2</code> optional fields: track_history_column_list=<code>[col1]</code>, track_history_except_column_list=<code>[col2]</code></td></tr><tr><td style=text-align:center>bronze_table_path_{env}</td><td style=text-align:left>Bronze table storage path.</td></tr><tr><td style=text-align:center>bronze_table_properties</td><td style=text-align:left>DLT table properties map. e.g. <code>{"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" }</code></td></tr><tr><td style=text-align:center>bronze_data_quality_expectations_json</td><td style=text-align:left>Bronze table data quality expectations</td></tr><tr><td style=text-align:center>bronze_database_quarantine_{env}</td><td style=text-align:left>Bronze database for quarantine data which fails expectations.</td></tr><tr><td style=text-align:center>bronze_quarantine_table Bronze</td><td style=text-align:left>Table for quarantine data which fails expectations</td></tr><tr><td style=text-align:center>bronze_quarantine_table_path_{env}</td><td style=text-align:left>Bronze database for quarantine data which fails expectations.</td></tr><tr><td style=text-align:center>bronze_quarantine_table_partitions</td><td style=text-align:left>Bronze quarantine tables partition cols</td></tr><tr><td style=text-align:center>bronze_quarantine_table_cluster_by</td><td style=text-align:left>Bronze quarantine tables cluster cols</td></tr><tr><td style=text-align:center>bronze_quarantine_table_properties</td><td style=text-align:left>DLT table properties map. e.g. <code>{"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false" }</code></td></tr><tr><td style=text-align:center>bronze_append_flows</td><td style=text-align:left>Bronze table append flows json. e.g.<code>"bronze_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}]</code></td></tr><tr><td style=text-align:center>silver_database_{env}</td><td style=text-align:left>Silver database name.</td></tr><tr><td style=text-align:center>silver_table</td><td style=text-align:left>Silver table name</td></tr><tr><td style=text-align:center>silver_partition_columns</td><td style=text-align:left>Silver table partition columns list</td></tr><tr><td style=text-align:center>silver_cluster_by</td><td style=text-align:left>Silver tables cluster by cols list</td></tr><tr><td style=text-align:center>silver_cdc_apply_changes</td><td style=text-align:left>Silver cdc apply changes Json</td></tr><tr><td style=text-align:center>silver_table_path_{env}</td><td style=text-align:left>Silver table storage path.</td></tr><tr><td style=text-align:center>silver_table_properties</td><td style=text-align:left>DLT table properties map. e.g. <code>{"pipelines.autoOptimize.managed": "false" , "pipelines.autoOptimize.zOrderCols": "year,month", "pipelines.reset.allowed": "false"}</code></td></tr><tr><td style=text-align:center>silver_transformation_json</td><td style=text-align:left>Silver table sql transformation json path</td></tr><tr><td style=text-align:center>silver_data_quality_expectations_json_{env}</td><td style=text-align:left>Silver table data quality expectations json file path</td></tr><tr><td style=text-align:center>silver_append_flows</td><td style=text-align:left>Silver table append flows json. e.g.<code>"silver_append_flows":[{"name":"customer_bronze_flow", "create_streaming_table": false,"source_format": "cloudFiles", "source_details": {"source_database": "APP","source_table":"CUSTOMERS", "source_path_dev": "tests/resources/data/customers", "source_schema_path": "tests/resources/schema/customer_schema.ddl"},"reader_options": {"cloudFiles.format": "json","cloudFiles.inferColumnTypes": "true","cloudFiles.rescuedDataColumn": "_rescued_data"},"once": true}]</code></td></tr></tbody></table><h3 id=data-quality-rules-file-structureexampleshttpsgithubcomdatabrickslabsdlt-metatreemainexamplesdqe>Data Quality Rules File Structure(<a href=https://github.com/databrickslabs/dlt-meta/tree/main/examples/dqe rel=external target=_blank>Examples</a>)</h3><table><thead><tr><th style=text-align:center>Field</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:center>expect</td><td style=text-align:left>Specify multiple data quality sql for each field when records that fail validation should be included in the target dataset</td></tr><tr><td style=text-align:center>expect_or_fail</td><td style=text-align:left>Specify multiple data quality sql for each field when records that fail validation should halt pipeline execution</td></tr><tr><td style=text-align:center>expect_or_drop</td><td style=text-align:left>Specify multiple data quality sql for each field when records that fail validation should be dropped from the target dataset</td></tr><tr><td style=text-align:center>expect_or_quarantine</td><td style=text-align:left>Specify multiple data quality sql for each field when records that fails validation will be dropped from main table and inserted into quarantine table specified in dataflowspec (only applicable for Bronze layer)</td></tr></tbody></table><h3 id=silver-transformation-file-structureexamplehttpsgithubcomdatabrickslabsdlt-metablobmainexamplessilver_transformationsjson>Silver transformation File Structure(<a href=https://github.com/databrickslabs/dlt-meta/blob/main/examples/silver_transformations.json rel=external target=_blank>Example</a>)</h3><table><thead><tr><th style=text-align:center>Field</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:center>target_table</td><td style=text-align:left>Specify target table name : Type String</td></tr><tr><td style=text-align:center>target_partition_cols</td><td style=text-align:left>Specify partition columns : Type Array</td></tr><tr><td style=text-align:center>select_exp</td><td style=text-align:left>Specify SQL expressions : Type Array</td></tr><tr><td style=text-align:center>where_clause</td><td style=text-align:left>Specify filter conditions if you want to prevent certain records from main input : Type Array</td></tr></tbody></table><footer class=footline></footer></article></div></main></div><aside id=R-sidebar class=default-animation><div id=R-header-topbar class=default-animation></div><div id=R-header-wrapper class=default-animation><div id=R-header class=default-animation><a id=logo href=https://databrickslabs.github.io/dlt-meta/></a></div><script>window.index_js_url="https://databrickslabs.github.io/dlt-meta/searchindex.en.js?1739407516"</script><search><form action=https://databrickslabs.github.io/dlt-meta/search/index.html method=get><div class="searchbox default-animation"><button class=search-detail type=submit title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
<label class=a11y-only for=R-search-by>Search</label>
<input data-search-input id=R-search-by name=search-by class=search-by type=search placeholder=Search...>
<button class=search-clear type=button data-search-clear title="Clear search"><i class="fas fa-times" title="Clear search"></i></button></div></form></search><script>var contentLangs=["en"]</script><script src=https://databrickslabs.github.io/dlt-meta/js/auto-complete.js?1739407516 defer></script><script src=https://databrickslabs.github.io/dlt-meta/js/lunr/lunr.min.js?1739407516 defer></script><script src=https://databrickslabs.github.io/dlt-meta/js/lunr/lunr.stemmer.support.min.js?1739407516 defer></script><script src=https://databrickslabs.github.io/dlt-meta/js/lunr/lunr.multi.min.js?1739407516 defer></script><script src=https://databrickslabs.github.io/dlt-meta/js/lunr/lunr.en.min.js?1739407516 defer></script><script src=https://databrickslabs.github.io/dlt-meta/js/search.js?1739407516 defer></script></div><div id=R-homelinks class="default-animation homelinks"><ul><li><a class=padding href=https://databrickslabs.github.io/dlt-meta/index.html><i class="fa-fw fas fa-home"></i> Home</a></li></ul><hr class=padding></div><div id=R-content-wrapper class=highlightable><div id=R-shortcutmenu-home class=R-sidebarmenu><ul class="enlarge morespace collapsible-menu"><li class=parent data-nav-id=/getting_started/index.html><a class=padding href=https://databrickslabs.github.io/dlt-meta/getting_started/index.html>Getting Started</a><ul id=R-subsections-d1738ab40ac858ee010dd1d061d67a7f class=collapsible-menu><li class=active data-nav-id=/getting_started/metadatapreperation/index.html><a class=padding href=https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/index.html>Metadata Preparation</a></li><li data-nav-id=/getting_started/dltmeta_cli/index.html><a class=padding href=https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_cli/index.html>DLT-META CLI</a></li><li data-nav-id=/getting_started/dltmeta_manual/index.html><a class=padding href=https://databrickslabs.github.io/dlt-meta/getting_started/dltmeta_manual/index.html>DLT-META Manual</a></li></ul></li><li data-nav-id=/demo/index.html><a class=padding href=https://databrickslabs.github.io/dlt-meta/demo/index.html>Demo</a><ul id=R-subsections-53a21a31163cea6582840a4862f776cf class=collapsible-menu></ul></li><li data-nav-id=/additionals/index.html><a class=padding href=https://databrickslabs.github.io/dlt-meta/additionals/index.html>Additional</a><ul id=R-subsections-1f3a6cf9d973879a2216966b6cbb3667 class=collapsible-menu></ul></li><li data-nav-id=/faq/index.html><a class=padding href=https://databrickslabs.github.io/dlt-meta/faq/index.html>Frequently Asked Questions (FAQs)</a><ul id=R-subsections-24e4fc446616bf2bbeac7b1dba804bb8 class=collapsible-menu></ul></li><li data-nav-id=/contributing/index.html><a class=padding href=https://databrickslabs.github.io/dlt-meta/contributing/index.html>Contributing</a><ul id=R-subsections-0d02badb7422c7536b9ac4528048a4b1 class=collapsible-menu></ul></li><li data-nav-id=/releases/index.html><a class=padding href=https://databrickslabs.github.io/dlt-meta/releases/index.html>Releases</a></li></ul></div><div class="padding footermargin footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showFooter"></div><div id=R-menu-footer><hr class="padding default-animation footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showFooter"><div id=R-prefooter class="footerLangSwitch footerVariantSwitch footerVisitedLinks"><ul><li id=R-select-language-container class=footerLangSwitch><div class="padding menu-control"><i class="fa-fw fas fa-language"></i>
<span>&nbsp;</span><div class=control-style><label class=a11y-only for=R-select-language>Language</label>
<select id=R-select-language onchange="location=this.querySelector(this.value).dataset.url"><option id=R-select-language-en value=#R-select-language-en data-url=https://databrickslabs.github.io/dlt-meta/getting_started/metadatapreperation/index.html lang=en-us selected></option></select></div><div class=clear></div></div></li><li id=R-select-variant-container class=footerVariantSwitch><div class="padding menu-control"><i class="fa-fw fas fa-paint-brush"></i>
<span>&nbsp;</span><div class=control-style><label class=a11y-only for=R-select-variant>Theme</label>
<select id=R-select-variant onchange=window.relearn.changeVariant(this.value)><option id=R-select-variant-learn value=learn selected>Learn</option></select></div><div class=clear></div></div><script>window.relearn.markVariant()</script></li><li class=footerVisitedLinks><div class="padding menu-control"><i class="fa-fw fas fa-history"></i>
<span>&nbsp;</span><div class=control-style><button onclick=clearHistory()>Clear History</button></div><div class=clear></div></div></li></ul></div><div id=R-footer class="footerFooter showFooter"><center><a class=github-button href=https://github.com/databrickslabs/dlt-meta/subscription data-icon=octicon-eye data-show-count=true aria-label="DLT-META on GitHub">Watch</a>
<a class=github-button href=https://github.com/databrickslabs/dlt-meta data-icon=octicon-star data-show-count=true aria-label="Star databricks/dlt-meta on GitHub">Star</a>
<a class=github-button href=https://github.com/databrickslabs/dlt-meta/fork data-icon=octicon-repo-forked data-show-count=true aria-label="Fork DLT-META on GitHub">Fork</a><p>A <a href=https://github.com/databrickslabs>Databricks Labs</a> project.</p></center><script async defer src=https://buttons.github.io/buttons.js></script></div></div></div></aside><script src=https://databrickslabs.github.io/dlt-meta/js/clipboard.min.js?1739407516 defer></script><script src=https://databrickslabs.github.io/dlt-meta/js/perfect-scrollbar.min.js?1739407516 defer></script><script src=https://databrickslabs.github.io/dlt-meta/js/theme.js?1739407516 defer></script></body></html>