var relearn_searchindex = [
  {
    "breadcrumb": "DLT-META \u003e Contributing",
    "content": "This document aims to provide complete information needed for anyone who would like to contribute to the dlt-meta project. Your contributions are vital to its success, whether you’re fixing bugs, improving documentation, or adding new features.\nSteps Step 0 - Read the documentation Refer documentation wiki page here that will guide you to access different DLT-META resources like documentation, github repo, presentation etc. Read the getting started link here to understand pre-requisite , setup steps and configuration details.\nPrerequisite\nInstall Databricks CLI to you local machine Authenticate you current machine to a Databricks Workspace Python 3.8.0+ Step 1 - Fork the Repository In case you may not be able to fork this repo because the repository is outside of your enterprise Databricks(EMU) , follow step3 or Fork using a personal github account.\nStep 2 - Clone the Repository Locally Run command “git clone https://github.com/databrickslabs/dlt-meta.git” it will create folder name “dlt-meta” Step 3 - Set Up the Development Environment cd dlt-meta Create python virtual environment python -m venv .venv or python3 -m venv .venv Activate python virtual environment source .venv/bin/activate Install databricks sdk pip install databricks-sdk Install code editor like VS code or any other. Import project into VS code File \u003e Open folder \u003e select above dlt-meta folder from your system Install setuptools and wheel if not already installed pip install setuptools wheel Install the project dependencies specified in setup.py pip install -e . Build the project\n* python setup.py sdist bdist_wheel Install additional dependencies pip install pyspark pip install delta-spark Pip install pytest Step 4 - Running Unit and Integration Tests Unit test are at tests folder\nTo run the test cases, use the pytest command in the terminal To run all tests run - pytest To run specific test- pytest -k “test_case_name” Integration Tests are at integration_tests folder\nTo run integration test run file run_integration_tests.py with mandatory required argument as below e.g. run_integration_tests.py --uc_catalog_name datta_demo --cloud_provider_name aws --dbr_version 14.3 --source cloudfiles --dbfs_path “dbfs:/tmp/DLT-META/” --profile DEFAULT Step 5 - Find Beginner-Friendly Issues Refer open issues [here](https://github.com/databrickslabs/dlt-meta/issues). Step 7 - Work on the Issue To work on the issue Fork the repository as shown below\nHere’s how to fork a repository on GitHub: Go to the repository’s page Click the Fork button A forked copy will be added to your GitHub repositories list A small text below the repository name will confirm that it’s a fork\nYou can also fork a repository on GitHub Desktop: Click Clone Repository in the File menu Select the local directory to clone the repository into Click Continue Comments the proposed sketch design which includes points problem statement, goals and objectives, proposed sketch, scope, implementation plan, risk and mitigation strategies and testing and validation plan. Write the code. Write the unit tests and validate them. Run and validate the integration test cases. Upon successful completion of testing, commit the changes to your git repository Step 8 - Submit a PR Once you have successfully made the changes, commit them to your repository and create pull requests. For more details, refer to the [documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork). Step 9 - Celebrate your Contribution Congratulations.",
    "description": "This document aims to provide complete information needed for anyone who would like to contribute to the dlt-meta project. Your contributions are vital to its success, whether you’re fixing bugs, improving documentation, or adding new features.\nSteps Step 0 - Read the documentation Refer documentation wiki page here that will guide you to access different DLT-META resources like documentation, github repo, presentation etc. Read the getting started link here to understand pre-requisite , setup steps and configuration details.",
    "tags": [],
    "title": "New Contributor Onboarding Guide",
    "uri": "/contributing/onboarding/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "The following tutorial will guide you through the process for setting up the DLT-META on your Databricks Lakehouse environment.\nYou will deploy/configure the solution, configure a database/table for bronze and silver layer as per below stages.",
    "description": "The following tutorial will guide you through the process for setting up the DLT-META on your Databricks Lakehouse environment.\nYou will deploy/configure the solution, configure a database/table for bronze and silver layer as per below stages.",
    "tags": [],
    "title": "Getting Started",
    "uri": "/getting_started/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Getting Started",
    "content": "Directory structure conf/ onboarding.json silver_transformations.json dqe/ bronze_data_quality_expectations.json Create onboarding.json Create silver_transformations.json Create data quality rules json’s for each entity e.g. Data Quality Rules The onboarding.json file contains links to silver_transformations.json and data quality expectation files dqe.\nonboarding.json File structure: Examples( Autoloader, Eventhub, Kafka ) env is your environment placeholder e.g dev, prod, stag\nField Description data_flow_id This is unique identifier for pipeline data_flow_group This is group identifier for launching multiple pipelines under single DLT source_format Source format e.g cloudFiles, eventhub, kafka, delta, snapshot source_details This map Type captures all source details for cloudfiles = source_schema_path, source_path_{env}, source_database, source_metadata For eventhub= source_schema_path , eventhub.accessKeyName, eventhub.accessKeySecretName, eventhub.name , eventhub.secretsScopeName , kafka.sasl.mechanism, kafka.security.protocol, eventhub.namespace, eventhub.port. For Source schema file spark DDL schema format parsing is supported In case of custom schema format then write schema parsing function bronze_schema_mapper(schema_file_path, spark):Schema and provide to OnboardDataflowspec initialization e.g onboardDataFlowSpecs = OnboardDataflowspec(spark, dict_obj,bronze_schema_mapper).onboardDataFlowSpecs(). For cloudFiles option _metadata columns addtiion there is source_metadata tag with attributes: include_autoloader_metadata_column flag (True or False value) will add _metadata column to target bronze dataframe, autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata,select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column. for snapshot= snapshot_format, source_path_{env} bronze_database_{env} Delta lake bronze database name. bronze_table Delta lake bronze table name bronze_reader_options Reader options which can be provided to spark reader e.g multiline=true,header=true in json format bronze_parition_columns Bronze table partition cols list bronze_cluster_by Bronze tables cluster by cols list bronze_cdc_apply_changes Bronze cdc apply changes Json bronze_apply_changes_from_snapshot Bronze apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] bronze_table_path_{env} Bronze table storage path. bronze_table_properties DLT table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\" } bronze_data_quality_expectations_json Bronze table data quality expectations bronze_database_quarantine_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table\tBronze Table for quarantine data which fails expectations bronze_quarantine_table_path_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table_partitions Bronze quarantine tables partition cols bronze_quarantine_table_cluster_by Bronze quarantine tables cluster cols bronze_quarantine_table_properties DLT table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\" } bronze_append_flows Bronze table append flows json. e.g.\"bronze_append_flows\":[{\"name\":\"customer_bronze_flow\", \"create_streaming_table\": false,\"source_format\": \"cloudFiles\", \"source_details\": {\"source_database\": \"APP\",\"source_table\":\"CUSTOMERS\", \"source_path_dev\": \"tests/resources/data/customers\", \"source_schema_path\": \"tests/resources/schema/customer_schema.ddl\"},\"reader_options\": {\"cloudFiles.format\": \"json\",\"cloudFiles.inferColumnTypes\": \"true\",\"cloudFiles.rescuedDataColumn\": \"_rescued_data\"},\"once\": true}] silver_database_{env} Silver database name. silver_table Silver table name silver_partition_columns Silver table partition columns list silver_cluster_by Silver tables cluster by cols list silver_cdc_apply_changes Silver cdc apply changes Json silver_table_path_{env} Silver table storage path. silver_table_properties DLT table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\"} silver_transformation_json Silver table sql transformation json path silver_data_quality_expectations_json_{env} Silver table data quality expectations json file path silver_append_flows Silver table append flows json. e.g.\"silver_append_flows\":[{\"name\":\"customer_bronze_flow\", \"create_streaming_table\": false,\"source_format\": \"cloudFiles\", \"source_details\": {\"source_database\": \"APP\",\"source_table\":\"CUSTOMERS\", \"source_path_dev\": \"tests/resources/data/customers\", \"source_schema_path\": \"tests/resources/schema/customer_schema.ddl\"},\"reader_options\": {\"cloudFiles.format\": \"json\",\"cloudFiles.inferColumnTypes\": \"true\",\"cloudFiles.rescuedDataColumn\": \"_rescued_data\"},\"once\": true}] Data Quality Rules File Structure(Examples) Field Description expect Specify multiple data quality sql for each field when records that fail validation should be included in the target dataset expect_or_fail Specify multiple data quality sql for each field when records that fail validation should halt pipeline execution expect_or_drop Specify multiple data quality sql for each field when records that fail validation should be dropped from the target dataset expect_or_quarantine Specify multiple data quality sql for each field when records that fails validation will be dropped from main table and inserted into quarantine table specified in dataflowspec (only applicable for Bronze layer) Silver transformation File Structure(Example) Field Description target_table Specify target table name : Type String target_partition_cols Specify partition columns : Type Array select_exp Specify SQL expressions : Type Array where_clause Specify filter conditions if you want to prevent certain records from main input : Type Array",
    "description": "Directory structure conf/ onboarding.json silver_transformations.json dqe/ bronze_data_quality_expectations.json Create onboarding.json Create silver_transformations.json Create data quality rules json’s for each entity e.g. Data Quality Rules The onboarding.json file contains links to silver_transformations.json and data quality expectation files dqe.\nonboarding.json File structure: Examples( Autoloader, Eventhub, Kafka ) env is your environment placeholder e.g dev, prod, stag\nField Description data_flow_id This is unique identifier for pipeline data_flow_group This is group identifier for launching multiple pipelines under single DLT source_format Source format e.g cloudFiles, eventhub, kafka, delta, snapshot source_details This map Type captures all source details for cloudfiles = source_schema_path, source_path_{env}, source_database, source_metadata For eventhub= source_schema_path , eventhub.accessKeyName, eventhub.accessKeySecretName, eventhub.name , eventhub.secretsScopeName , kafka.sasl.mechanism, kafka.security.protocol, eventhub.namespace, eventhub.port. For Source schema file spark DDL schema format parsing is supported In case of custom schema format then write schema parsing function bronze_schema_mapper(schema_file_path, spark):Schema and provide to OnboardDataflowspec initialization e.g onboardDataFlowSpecs = OnboardDataflowspec(spark, dict_obj,bronze_schema_mapper).onboardDataFlowSpecs(). For cloudFiles option _metadata columns addtiion there is source_metadata tag with attributes: include_autoloader_metadata_column flag (True or False value) will add _metadata column to target bronze dataframe, autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata,select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column. for snapshot= snapshot_format, source_path_{env} bronze_database_{env} Delta lake bronze database name. bronze_table Delta lake bronze table name bronze_reader_options Reader options which can be provided to spark reader e.g multiline=true,header=true in json format bronze_parition_columns Bronze table partition cols list bronze_cluster_by Bronze tables cluster by cols list bronze_cdc_apply_changes Bronze cdc apply changes Json bronze_apply_changes_from_snapshot Bronze apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] bronze_table_path_{env} Bronze table storage path. bronze_table_properties DLT table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\" } bronze_data_quality_expectations_json Bronze table data quality expectations bronze_database_quarantine_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table\tBronze Table for quarantine data which fails expectations bronze_quarantine_table_path_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table_partitions Bronze quarantine tables partition cols bronze_quarantine_table_cluster_by Bronze quarantine tables cluster cols bronze_quarantine_table_properties DLT table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\" } bronze_append_flows Bronze table append flows json. e.g.\"bronze_append_flows\":[{\"name\":\"customer_bronze_flow\", \"create_streaming_table\": false,\"source_format\": \"cloudFiles\", \"source_details\": {\"source_database\": \"APP\",\"source_table\":\"CUSTOMERS\", \"source_path_dev\": \"tests/resources/data/customers\", \"source_schema_path\": \"tests/resources/schema/customer_schema.ddl\"},\"reader_options\": {\"cloudFiles.format\": \"json\",\"cloudFiles.inferColumnTypes\": \"true\",\"cloudFiles.rescuedDataColumn\": \"_rescued_data\"},\"once\": true}] silver_database_{env} Silver database name. silver_table Silver table name silver_partition_columns Silver table partition columns list silver_cluster_by Silver tables cluster by cols list silver_cdc_apply_changes Silver cdc apply changes Json silver_table_path_{env} Silver table storage path. silver_table_properties DLT table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\"} silver_transformation_json Silver table sql transformation json path silver_data_quality_expectations_json_{env} Silver table data quality expectations json file path silver_append_flows Silver table append flows json. e.g.\"silver_append_flows\":[{\"name\":\"customer_bronze_flow\", \"create_streaming_table\": false,\"source_format\": \"cloudFiles\", \"source_details\": {\"source_database\": \"APP\",\"source_table\":\"CUSTOMERS\", \"source_path_dev\": \"tests/resources/data/customers\", \"source_schema_path\": \"tests/resources/schema/customer_schema.ddl\"},\"reader_options\": {\"cloudFiles.format\": \"json\",\"cloudFiles.inferColumnTypes\": \"true\",\"cloudFiles.rescuedDataColumn\": \"_rescued_data\"},\"once\": true}] Data Quality Rules File Structure(Examples) Field Description expect Specify multiple data quality sql for each field when records that fail validation should be included in the target dataset expect_or_fail Specify multiple data quality sql for each field when records that fail validation should halt pipeline execution expect_or_drop Specify multiple data quality sql for each field when records that fail validation should be dropped from the target dataset expect_or_quarantine Specify multiple data quality sql for each field when records that fails validation will be dropped from main table and inserted into quarantine table specified in dataflowspec (only applicable for Bronze layer) Silver transformation File Structure(Example) Field Description target_table Specify target table name : Type String target_partition_cols Specify partition columns : Type Array select_exp Specify SQL expressions : Type Array where_clause Specify filter conditions if you want to prevent certain records from main input : Type Array",
    "tags": [],
    "title": "Metadata Preparation",
    "uri": "/getting_started/metadatapreperation/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Getting Started",
    "content": "pre-requisites: Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace:\ndatabricks auth login --host WORKSPACE_HOST Python 3.8.0 +\nSteps: git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk OnboardJob Run Onboarding using dlt-meta cli command: databricks labs dlt-meta onboard Above command will prompt you to provide onboarding details. If you have cloned dlt-meta git repo then accepting defaults will launch config from demo/conf folder. You can create onboarding files e.g onboarding.json, data quality and silver transformations and put it in conf folder as show in demo/conf Once onboarding jobs is finished deploy bronze and silver DLT using below command Dataflow DLT Pipeline: Deploy Bronze DLT databricks labs dlt-meta deploy Above command will prompt you to provide dlt details. Please provide respective details for schema which you provided in above steps Deploy Silver DLT databricks labs dlt-meta deploy Above command will prompt you to provide dlt details. Please provide respective details for schema which you provided in above steps Goto your databricks workspace and located onboarding job under: Workflow-\u003eJobs runs",
    "description": "pre-requisites: Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace:\ndatabricks auth login --host WORKSPACE_HOST Python 3.8.0 +\nSteps: git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk OnboardJob Run Onboarding using dlt-meta cli command: databricks labs dlt-meta onboard Above command will prompt you to provide onboarding details. If you have cloned dlt-meta git repo then accepting defaults will launch config from demo/conf folder. You can create onboarding files e.g onboarding.json, data quality and silver transformations and put it in conf folder as show in demo/conf",
    "tags": [],
    "title": "DLT-META CLI",
    "uri": "/getting_started/dltmeta_cli/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Getting Started",
    "content": "OnboardJob Option#1: using Databricks Python whl job Go to your Databricks landing page and do one of the following:\nIn the sidebar, click Jobs Icon Workflows and click Create Job Button.\nIn the sidebar, click New Icon New and select Job from the menu.\nIn the task dialog box that appears on the Tasks tab, replace Add a name for your job… with your job name, for example, Python wheel example.\nIn Task name, enter a name for the task, for example, dlt_meta_onboarding_pythonwheel_task.\nIn Type, select Python wheel.\nIn Package name, enter dlt_meta.\nIn Entry point, enter run.\nClick Add under Dependent Libraries. In the Add dependent library dialog, under Library Type, click PyPI. Enter Package = dlt-meta\nClick Add.\nIn Parameters, select keyword argument then select JSON. Past below json parameters with :\nWithout Unity Cataglog { \"onboard_layer\": \"bronze_silver\", \"database\": \"dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"silver_dataflowspec_path\": \"dbfs:/onboarding_tables_cdc/silver\", \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"import_author\": \"Ravi\", \"version\": \"v1\", \"bronze_dataflowspec_path\": \"dbfs:/onboarding_tables_cdc/bronze\", \"onboard_layer\": \"bronze_silver\", \"uc_enabled\": \"False\", \"overwrite\": \"True\", \"env\": \"dev\" } with Unity catalog { \"onboard_layer\": \"bronze_silver\", \"database\": \"uc_name.dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"import_author\": \"Ravi\", \"version\": \"v1\", \"uc_enabled\": \"True\", \"overwrite\": \"True\", \"env\": \"dev\" } Note in database field you need to provide catalog name then schema name as \u003c\u003cuc_name\u003e\u003e.\u003c\u003cschema\u003e\u003e Alternatly you can enter keyword arguments, click + Add and enter a key and value. Click + Add again to enter more arguments.\nClick Save task.\nRun now\nMake sure job run successfully. Verify metadata in your dataflow spec tables entered in step: 11 e.g dlt_demo.bronze_dataflowspec_table , dlt_demo.silver_dataflowspec_table\nOption#2: Databricks Notebook Copy below code to databricks notebook cells %pip install dlt-meta without unity catalog onboarding_params_map = { \"database\": \"dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"bronze_dataflowspec_path\": \"dbfs:/onboarding_tables_cdc/bronze\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"silver_dataflowspec_path\": \"dbfs:/onboarding_tables_cdc/silver\", \"overwrite\": \"True\", \"env\": \"dev\", \"version\": \"v1\", \"import_author\": \"Ravi\" } from src.onboard_dataflowspec import OnboardDataflowspec OnboardDataflowspec(spark, onboarding_params_map).onboard_dataflow_specs() with unity catalog onboarding_params_map = { \"database\": \"uc_name.dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\",, \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"overwrite\": \"True\", \"env\": \"dev\", \"version\": \"v1\", \"import_author\": \"Ravi\" } from src.onboard_dataflowspec import OnboardDataflowspec OnboardDataflowspec(spark, onboarding_params_map, uc_enabled=True).onboard_dataflow_specs() Specify your onboarding config params in above onboarding_params_map\nRun notebook cells\nDataflow DLT Pipeline: Delta Live Tables launch notebook Go to your Databricks landing page and select Create a notebook, or click New Icon New in the sidebar and select Notebook. The Create Notebook dialog appears.\nIn the Create Notebook dialogue, give your notebook a name e.g dlt_meta_pipeline and select Python from the Default Language dropdown menu. You can leave Cluster set to the default value. The Delta Live Tables runtime creates a cluster before it runs your pipeline.\nClick Create.\nYou can add the example dlt pipeline code or import iPython notebook as is.\n%pip install dlt-meta layer = spark.conf.get(\"layer\", None) from src.dataflow_pipeline import DataflowPipeline DataflowPipeline.invoke_dlt_pipeline(spark, layer) Create Bronze DLT pipeline Click Jobs Icon Workflows in the sidebar, click the Delta Live Tables tab, and click Create Pipeline.\nGive the pipeline a name e.g. DLT_META_BRONZE and click File Picker Icon to select a notebook dlt_meta_pipeline created in step: Create a dlt launch notebook.\nOptionally enter a storage location for output data from the pipeline. The system uses a default location if you leave Storage location empty.\nSelect Triggered for Pipeline Mode.\nEnter Configuration parameters e.g.\n\"layer\": \"bronze\", \"bronze.dataflowspecTable\": \"dataflowspec table name\", \"bronze.group\": \"enter group name from metadata e.g. G1\", Enter target schema where you wants your bronze tables to be created\nClick Create.\nStart pipeline: click the Start button on in top panel. The system returns a message confirming that your pipeline is starting\nCreate Silver DLT pipeline Click Jobs Icon Workflows in the sidebar, click the Delta Live Tables tab, and click Create Pipeline.\nGive the pipeline a name e.g. DLT_META_SILVER and click File Picker Icon to select a notebook dlt_meta_pipeline created in step: Create a dlt launch notebook.\nOptionally enter a storage location for output data from the pipeline. The system uses a default location if you leave Storage location empty.\nSelect Triggered for Pipeline Mode.\nEnter Configuration parameters e.g.\n\"layer\": \"silver\", \"silver.dataflowspecTable\": \"dataflowspec table name\", \"silver.group\": \"enter group name from metadata e.g. G1\", Enter target schema where you wants your silver tables to be created\nClick Create.\nStart pipeline: click the Start button on in top panel. The system returns a message confirming that your pipeline is starting",
    "description": "OnboardJob Option#1: using Databricks Python whl job Go to your Databricks landing page and do one of the following:\nIn the sidebar, click Jobs Icon Workflows and click Create Job Button.\nIn the sidebar, click New Icon New and select Job from the menu.\nIn the task dialog box that appears on the Tasks tab, replace Add a name for your job… with your job name, for example, Python wheel example.",
    "tags": [],
    "title": "DLT-META Manual",
    "uri": "/getting_started/dltmeta_manual/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "DAIS 2023 DEMO: Showcases DLT-META’s capabilities of creating Bronze and Silver DLT pipelines with initial and incremental mode automatically. Databricks Techsummit Demo: 100s of data sources ingestion in bronze and silver DLT pipelines automatically. Append FLOW Autoloader Demo: Write to same target from multiple sources using append_flow and adding file metadata using File metadata column Append FLOW Eventhub Demo: Write to same target from multiple sources using append_flow and adding using File metadata column Silver Fanout Demo: This demo will showcase fanout architecture can be implemented in silver layer Apply Changes From Snapshot Demo: This demo will showcase apply_changes_from_snapshot can be implemented inside bronze layer",
    "description": "DAIS 2023 DEMO: Showcases DLT-META’s capabilities of creating Bronze and Silver DLT pipelines with initial and incremental mode automatically. Databricks Techsummit Demo: 100s of data sources ingestion in bronze and silver DLT pipelines automatically. Append FLOW Autoloader Demo: Write to same target from multiple sources using append_flow and adding file metadata using File metadata column Append FLOW Eventhub Demo: Write to same target from multiple sources using append_flow and adding using File metadata column Silver Fanout Demo: This demo will showcase fanout architecture can be implemented in silver layer Apply Changes From Snapshot Demo: This demo will showcase apply_changes_from_snapshot can be implemented inside bronze layer",
    "tags": [],
    "title": "Demo",
    "uri": "/demo/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "DAIS 2023 DEMO: DAIS 2023 Session Recording This demo showcases DLT-META’s capabilities of creating Bronze and Silver DLT pipelines with initial and incremental mode automatically.\nCustomer and Transactions feeds for initial load Adds new feeds Product and Stores to existing Bronze and Silver DLT pipelines with metadata changes. Runs Bronze and Silver DLT for incremental load for CDC events Steps to launch DAIS demo in your Databricks workspace: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home python demo/launch_dais_demo.py --uc_catalog_name=\u003c\u003cuc catalog name\u003e\u003e --cloud_provider_name=\u003c\u003c\u003e\u003e uc_catalog_name : unit catalog name cloud_provider_name : aws or azure or gcp you can provide --profile=databricks_profile name in case you already have databricks cli otherwise command prompt will ask host and token.",
    "description": "DAIS 2023 DEMO: DAIS 2023 Session Recording This demo showcases DLT-META’s capabilities of creating Bronze and Silver DLT pipelines with initial and incremental mode automatically.\nCustomer and Transactions feeds for initial load Adds new feeds Product and Stores to existing Bronze and Silver DLT pipelines with metadata changes. Runs Bronze and Silver DLT for incremental load for CDC events Steps to launch DAIS demo in your Databricks workspace: Launch Command Prompt",
    "tags": [],
    "title": "DAIS DEMO",
    "uri": "/demo/dais/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Databricks Tech Summit FY2024 DEMO: This demo will launch auto generated tables(100s) inside single bronze and silver DLT pipeline using dlt-meta.\nLaunch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run the command python demo/launch_techsummit_demo.py --uc_catalog_name=\u003c\u003cUnity Catalog name\u003e\u003e --cloud_provider_name=aws uc_catalog_name : Unity Catalog name cloud_provider_name : aws or azure you can provide --profile=databricks_profile name in case you already have databricks cli otherwise command prompt will ask host and token",
    "description": "Databricks Tech Summit FY2024 DEMO: This demo will launch auto generated tables(100s) inside single bronze and silver DLT pipeline using dlt-meta.\nLaunch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run the command python demo/launch_techsummit_demo.py --uc_catalog_name=\u003c\u003cUnity Catalog name\u003e\u003e --cloud_provider_name=aws",
    "tags": [],
    "title": "Tech Summit DEMO",
    "uri": "/demo/techsummit/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Append FLOW Autoloader Demo: This demo will perform following tasks:\nRead from different source paths using autoloader and write to same target using dlt.append_flow API Read from different delta tables and write to same silver table using append_flow API Add file_name and file_path to target bronze table for autoloader source using File metadata column Append flow with autoloader Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home python demo/launch_af_cloudfiles_demo.py --cloud_provider_name=aws --dbr_version=15.3.x-scala2.12 --dbfs_path=dbfs:/tmp/DLT-META/demo/ --uc_catalog_name=dlt_meta_uc cloud_provider_name : aws or azure or gcp db_version : Databricks Runtime Version dbfs_path : Path on your Databricks workspace where demo will be copied for launching DLT-META Pipelines uc_catalog_name: Unity catalog name you can provide --profile=databricks_profile name in case you already have databricks cli otherwise command prompt will ask host and token",
    "description": "Append FLOW Autoloader Demo: This demo will perform following tasks:\nRead from different source paths using autoloader and write to same target using dlt.append_flow API Read from different delta tables and write to same silver table using append_flow API Add file_name and file_path to target bronze table for autoloader source using File metadata column Append flow with autoloader Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal",
    "tags": [],
    "title": "Append FLOW Autoloader Demo",
    "uri": "/demo/append_flow_cf/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Append FLOW Autoloader Demo: Read from different eventhub topics and write to same target tables using dlt.append_flow API Steps: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Eventhub\nNeeds eventhub instance running\nNeed two eventhub topics first for main feed (eventhub_name) and second for append flow feed (eventhub_name_append_flow)\nCreate databricks secrets scope for eventhub keys\ncommandline databricks secrets create-scope eventhubs_dltmeta_creds databricks secrets put-secret --json '{ \"scope\": \"eventhubs_dltmeta_creds\", \"key\": \"RootManageSharedAccessKey\", \"string_value\": \"\u003c\u003cvalue\u003e\u003e\" }' Create databricks secrets to store producer and consumer keys using the scope created in step 2\nFollowing are the mandatory arguments for running EventHubs demo\ncloud_provider_name: Cloud provider name e.g. aws or azure dbr_version: Databricks Runtime Version e.g. 15.3.x-scala2.12 uc_catalog_name : unity catalog name e.g. dlt_meta_uc dbfs_path: Path on your Databricks workspace where demo will be copied for launching DLT-META Pipelines e.g. dbfs:/tmp/DLT-META/demo/ eventhub_namespace: Eventhub namespace e.g. dltmeta eventhub_name : Primary Eventhubname e.g. dltmeta_demo eventhub_name_append_flow: Secondary eventhub name for appendflow feed e.g. dltmeta_demo_af eventhub_producer_accesskey_name: Producer databricks access keyname e.g. RootManageSharedAccessKey eventhub_consumer_accesskey_name: Consumer databricks access keyname e.g. RootManageSharedAccessKey eventhub_secrets_scope_name: Databricks secret scope name e.g. eventhubs_dltmeta_creds eventhub_port: Eventhub port python demo/launch_af_eventhub_demo.py --cloud_provider_name=aws --uc_catalog_name=dlt_meta_uc --eventhub_name=dltmeta_demo --eventhub_name_append_flow=dltmeta_demo_af --eventhub_secrets_scope_name=dltmeta_eventhub_creds --eventhub_namespace=dltmeta --eventhub_port=9093 --eventhub_producer_accesskey_name=RootManageSharedAccessKey --eventhub_consumer_accesskey_name=RootManageSharedAccessKey --eventhub_accesskey_secret_name=RootManageSharedAccessKey",
    "description": "Append FLOW Autoloader Demo: Read from different eventhub topics and write to same target tables using dlt.append_flow API Steps: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Eventhub\nNeeds eventhub instance running\nNeed two eventhub topics first for main feed (eventhub_name) and second for append flow feed (eventhub_name_append_flow)",
    "tags": [],
    "title": "Append FLOW Eventhub Demo",
    "uri": "/demo/append_flow_eh/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Silver Fanout Demo This demo will perform following steps Showcase onboarding process for silver fanout pattern Run onboarding for the bronze cars table, which contains data from various countries. Run onboarding for the silver tables, which have a where_clause based on the country condition in silver_transformations_cars.json. Run Bronze for cars tables Run onboarding for the silver tables, fanning out from the bronze cars tables to country-specific tables such as cars_usa, cars_uk, cars_germany, and cars_japan. Steps: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home python demo/launch_silver_fanout_demo.py --uc_catalog_name=\u003c\u003cuc catalog name\u003e\u003e --cloud_provider_name=aws uc_catalog_name : aws or azure\ncloud_provider_name : aws or azure\nyou can provide --profile=databricks_profile name in case you already have databricks cli otherwise command prompt will ask host and token.\n6a. Databricks Workspace URL: Enter your workspace URL, with the format https://.cloud.databricks.com. To get your workspace URL, see Workspace instance names, URLs, and IDs. 6b. Token: In your Databricks workspace, click your Databricks username in the top bar, and then select User Settings from the drop down.\nOn the Access tokens tab, click Generate new token.\n(Optional) Enter a comment that helps you to identify this token in the future, and change the token’s default lifetime of 90 days. To create a token with no lifetime (not recommended), leave the Lifetime (days) box empty (blank).\nClick Generate.\nCopy the displayed token\nPaste to command prompt",
    "description": "Silver Fanout Demo This demo will perform following steps Showcase onboarding process for silver fanout pattern Run onboarding for the bronze cars table, which contains data from various countries. Run onboarding for the silver tables, which have a where_clause based on the country condition in silver_transformations_cars.json. Run Bronze for cars tables Run onboarding for the silver tables, fanning out from the bronze cars tables to country-specific tables such as cars_usa, cars_uk, cars_germany, and cars_japan. Steps: Launch Command Prompt",
    "tags": [],
    "title": "Silver Fanout Demo",
    "uri": "/demo/silver_fanout/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Apply Changes From Snapshot Demo This demo will perform following steps Showcase onboarding process for apply changes from snapshot pattern Run onboarding for the bronze stores and products tables, which contains data snapshot data in csv files. Run Bronze DLT to load initial snapshot (LOAD_1.csv) Upload incremental snapshot LOAD_2.csv version=2 for stores and product Run Bronze DLT to load incremental snapshot (LOAD_2.csv). Stores is scd_type=2 so updated records will expired and added new records with version_number. Products is scd_type=1 so in case records missing for scd_type=1 will be deleted. Upload incremental snapshot LOAD_3.csv version=3 for stores and product Run Bronze DLT to load incremental snapshot (LOAD_3.csv). Stores is scd_type=2 so updated records will expired and added new records with version_number. Products is scd_type=1 so in case records missing for scd_type=1 will be deleted. Steps: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home python demo/launch_acfs_demo.py --uc_catalog_name=\u003c\u003cuc catalog name\u003e\u003e uc_catalog_name : Unity catalog name you can provide --profile=databricks_profile name in case you already have databricks cli otherwise command prompt will ask host and token.",
    "description": "Apply Changes From Snapshot Demo This demo will perform following steps Showcase onboarding process for apply changes from snapshot pattern Run onboarding for the bronze stores and products tables, which contains data snapshot data in csv files. Run Bronze DLT to load initial snapshot (LOAD_1.csv) Upload incremental snapshot LOAD_2.csv version=2 for stores and product Run Bronze DLT to load incremental snapshot (LOAD_2.csv). Stores is scd_type=2 so updated records will expired and added new records with version_number. Products is scd_type=1 so in case records missing for scd_type=1 will be deleted. Upload incremental snapshot LOAD_3.csv version=3 for stores and product Run Bronze DLT to load incremental snapshot (LOAD_3.csv). Stores is scd_type=2 so updated records will expired and added new records with version_number. Products is scd_type=1 so in case records missing for scd_type=1 will be deleted. Steps: Launch Command Prompt",
    "tags": [],
    "title": "Silver Fanout Demo",
    "uri": "/demo/apply_changes_from_snapshot/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "Integration Tests: To demonstrate DLT-META’s integration tests and CI/CD capabilities using the Databricks SDK, you can follow these steps. These steps involve setting up a testing environment, writing integration tests, and configuring a CI/CD pipeline to run these tests",
    "description": "Integration Tests: To demonstrate DLT-META’s integration tests and CI/CD capabilities using the Databricks SDK, you can follow these steps. These steps involve setting up a testing environment, writing integration tests, and configuring a CI/CD pipeline to run these tests",
    "tags": [],
    "title": "Additional",
    "uri": "/additionals/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Additional",
    "content": "Run Integration Tests Initial steps Prerequisite: Datatbricks CLI installed as given here git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk dlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run integration test against cloudfile or eventhub or kafka using below options: If databricks profile configured using CLI then pass --profile \u003cprofile-name\u003e to below command otherwise provide workspace url and token in command line\n2a. Run the command for cloudfiles python integration_tests/run_integration_tests.py --source=cloudfiles uc_catalog_name=\u003c\u003c\u003e\u003e\n2b. Run the command for eventhub python integration-tests/run_integration_tests.py --cloud_provider_name=azure --dbr_version=15.3.x-scala2.12 --source=eventhub --dbfs_path=dbfs:/tmp/DLT-META/ --eventhub_name=iot --eventhub_secrets_scope_name=eventhubs_creds --eventhub_namespace=int_test-standard --eventhub_port=9093 --eventhub_producer_accesskey_name=producer --eventhub_consumer_accesskey_name=consumer\nFor eventhub integration tests, the following are the prerequisites: Needs eventhub instance running Use Databricks CLI, Create databricks secrets scope for eventhub keys (databricks secrets create-scope eventhubs_creds) Use Databricks CLI, Create databricks secrets to store producer and consumer keys using the scope created in step Following are the mandatory arguments for running EventHubs integration test Provide your eventhub topic : –eventhub_name Provide eventhub namespace : –eventhub_namespace Provide eventhub port : –eventhub_port Provide databricks secret scope name : –eventhub_secrets_scope_name Provide eventhub producer access key name : –eventhub_producer_accesskey_name Provide eventhub access key name : –eventhub_consumer_accesskey_name 2c. Run the command for kafka python3 integration_tests/run_integration_tests.py --cloud_provider_name=aws --dbr_version=15.3.x-scala2.12 --source=kafka --dbfs_path=dbfs:/tmp/DLT-META/ --kafka_topic_name=dlt-meta-integration-test --kafka_broker=host:9092\nFor kafka integration tests, the following are the prerequisites: Needs kafka instance running Following are the mandatory arguments for running EventHubs integration test Provide your kafka topic name : –kafka_topic_name Provide kafka_broker : –kafka_broker Once finished integration output file will be copied locally to integration-test-output_\u003crun_id\u003e.txt\nOutput of a successful run should have the following in the file\n,0 0,Completed Bronze DLT Pipeline. 1,Completed Silver DLT Pipeline. 2,Validating DLT Bronze and Silver Table Counts... 3,Validating Counts for Table bronze_7d1d3ccc9e144a85b07c23110ea50133.transactions. 4,Expected: 10002 Actual: 10002. Passed! 5,Validating Counts for Table bronze_7d1d3ccc9e144a85b07c23110ea50133.transactions_quarantine. 6,Expected: 7 Actual: 7. Passed! 7,Validating Counts for Table bronze_7d1d3ccc9e144a85b07c23110ea50133.customers. 8,Expected: 98928 Actual: 98928. Passed! 9,Validating Counts for Table bronze_7d1d3ccc9e144a85b07c23110ea50133.customers_quarantine. 10,Expected: 1077 Actual: 1077. Passed! 11,Validating Counts for Table silver_7d1d3ccc9e144a85b07c23110ea50133.transactions. 12,Expected: 8759 Actual: 8759. Passed! 13,Validating Counts for Table silver_7d1d3ccc9e144a85b07c23110ea50133.customers. 14,Expected: 87256 Actual: 87256. Passed!",
    "description": "Run Integration Tests Initial steps Prerequisite: Datatbricks CLI installed as given here git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk dlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run integration test against cloudfile or eventhub or kafka using below options: If databricks profile configured using CLI then pass --profile \u003cprofile-name\u003e to below command otherwise provide workspace url and token in command line",
    "tags": [],
    "title": "Integration Tests",
    "uri": "/additionals/integration_tests/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Frequently Asked Questions (FAQs)",
    "uri": "/faq/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Frequently Asked Questions (FAQs)",
    "content": "Q. How do I get started ?\nPlease refer to the Getting Started guide\nQ. How do I create metadata DLT-META ?\nDLT-META needs following metadata files:\nOnboarding File captures input/output metadata Data Quality Rules File captures data quality rules Silver transformation File captures processing logic as sql Q. What is DataflowSpecs?\nDLT-META translates input metadata into Delta table as DataflowSpecs\nQ. How many DLT pipelines will be launched using DLT-META?\nDLT-META uses data_flow_group to launch DLT pipelines, so all the tables belongs to same group will be executed under single DLT pipeline.\nQ. Can we run onboarding for bronze layer only?\nYes! Please follow below steps:\nBronze Metadata preparation (example) Onboarding Job Option#1: DLT-META CLI Option#2: Manual Job Use below parameters { \"onboard_layer\": \"bronze\", \"database\": \"dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"import_author\": \"Ravi\", \"version\": \"v1\", \"uc_enabled\": \"True\", \"overwrite\": \"True\", \"env\": \"dev\" } option#3: Databircks Notebook onboarding_params_map = { \"database\": \"uc_name.dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"overwrite\": \"True\", \"env\": \"dev\", \"version\": \"v1\", \"import_author\": \"Ravi\" } from src.onboard_dataflowspec import OnboardDataflowspec OnboardDataflowspec(spark, onboarding_params_map, uc_enabled=True).onboard_bronze_dataflow_spec() Q. Can we run onboarding for silver layer only? Yes! Please follow below steps:\nBronze Metadata preparation (example) Onboarding Job Option#1: DLT-META CLI Option#2: Manual Job Use below parameters { \"onboard_layer\": \"silver\", \"database\": \"dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"import_author\": \"Ravi\", \"version\": \"v1\", \"uc_enabled\": \"True\", \"overwrite\": \"True\", \"env\": \"dev\" } option#3: Databircks Notebook onboarding_params_map = { \"database\": \"uc_name.dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"overwrite\": \"True\", \"env\": \"dev\", \"version\": \"v1\", \"import_author\": \"Ravi\" } from src.onboard_dataflowspec import OnboardDataflowspec OnboardDataflowspec(spark, onboarding_params_map, uc_enabled=True).onboard_silver_dataflow_spec() Q. How to chain multiple silver tables after bronze table?\nExample: After customers_cdc bronze table, can I have customers silver table reading from customers_cdc and another customers_clean silver table reading from customers_cdc? If so, how do I define these in onboarding.json?\nYou can run onboarding for additional silver customer_clean table by having onboarding file and silver transformation with filter condition for fan out.\nRun onboarding for slilver layer in append mode(“overwrite”: “False”) so it will append to existing silver tables. When you launch DLT pipeline it will read silver onboarding and run DLT for bronze source and silver as target\nQ. How can I do type1 or type2 merge to target table?\nUsing DLT’s dlt.apply_changes we can do type1 or type2 merge. DLT-META have tag in onboarding file as bronze_cdc_apply_changes or silver_cdc_apply_changes which maps to DLT’s apply_changes API. \"silver_cdc_apply_changes\": { \"keys\":[ \"customer_id\" ], \"sequence_by\":\"dmsTimestamp\", \"scd_type\":\"2\", \"apply_as_deletes\":\"Op = 'D'\", \"except_column_list\":[ \"Op\", \"dmsTimestamp\", \"_rescued_data\" ] } Q. How can I write to same target table using different sources?\nUsing DLT’s dlt.append_flow API we can write to same target from different sources. DLT-META have tag in onboarding file as bronze_append_flows and silver_append_flows dlt.append_flow API is mapped to [ { \"name\":\"customer_bronze_flow1\", \"create_streaming_table\":false, \"source_format\":\"cloudFiles\", \"source_details\":{ \"source_path_dev\":\"tests/resources/data/customers\", \"source_schema_path\":\"tests/resources/schema/customer_schema.ddl\" }, \"reader_options\":{ \"cloudFiles.format\":\"json\", \"cloudFiles.inferColumnTypes\":\"true\", \"cloudFiles.rescuedDataColumn\":\"_rescued_data\" }, \"once\":true }, { \"name\":\"customer_bronze_flow2\", \"create_streaming_table\":false, \"source_format\":\"delta\", \"source_details\":{ \"source_database\":\"{uc_catalog_name}.{bronze_schema}\", \"source_table\":\"customers_delta\" }, \"reader_options\":{ }, \"once\":false } ] Q. How to add autloaders file metadata to bronze table?\nDLT-META have tag source_metadata in onboarding json under source_details\n\"source_metadata\":{ \"include_autoloader_metadata_column\":\"True\", \"autoloader_metadata_col_name\":\"source_metadata\", \"select_metadata_cols\":{ \"input_file_name\":\"_metadata.file_name\", \"input_file_path\":\"_metadata.file_path\" } } include_autoloader_metadata_column flag will add _metadata column to target bronze dataframe. autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column",
    "description": "Q. How do I get started ?\nPlease refer to the Getting Started guide\nQ. How do I create metadata DLT-META ?\nDLT-META needs following metadata files:\nOnboarding File captures input/output metadata Data Quality Rules File captures data quality rules Silver transformation File captures processing logic as sql Q. What is DataflowSpecs?\nDLT-META translates input metadata into Delta table as DataflowSpecs\nQ. How many DLT pipelines will be launched using DLT-META?",
    "tags": [],
    "title": "Execution",
    "uri": "/faq/execution/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Frequently Asked Questions (FAQs)",
    "content": "Q. What is DLT-META ?\nDLT-META is a solution/framework using Databricks Delta Live Tables aka DLT which helps you automate bronze and silver layer pipelines using CI/CD.\nQ. What are the benefits of using DLT-META ?\nWith DLT-META customers needs to only maintain metadata like onboarding.json, data quality rules and silver transformations and framework will take care of execution. In case of any input/output or data quality rules or silver transformation logic changes there will be only metadata changes using onboarding interface and no need to re-deploy pipelines. If you have 100s or 1000s of tables then DLT-META speeds up overall turn around time to production as customers needs to just produce metadata Q. What different types of reader are supported using DLT-META ?\nDLT-META uses Databricks Auto Loader, DELTA, KAFKA, EVENTHUB to read from s3/adls/blog storage.\nQ. Can DLT-META support any other readers?\nDLT-META can support any spark streaming reader. You can override read_bronze() api inside DataflowPipeline.py to support any reader\nQ. Who should use this framework ?\nData Engineering teams, who wants to automate data migrations at scale using CI/CD.",
    "description": "Q. What is DLT-META ?\nDLT-META is a solution/framework using Databricks Delta Live Tables aka DLT which helps you automate bronze and silver layer pipelines using CI/CD.\nQ. What are the benefits of using DLT-META ?\nWith DLT-META customers needs to only maintain metadata like onboarding.json, data quality rules and silver transformations and framework will take care of execution. In case of any input/output or data quality rules or silver transformation logic changes there will be only metadata changes using onboarding interface and no need to re-deploy pipelines. If you have 100s or 1000s of tables then DLT-META speeds up overall turn around time to production as customers needs to just produce metadata Q. What different types of reader are supported using DLT-META ?",
    "tags": [],
    "title": "General",
    "uri": "/faq/general/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "Contributing to DLT-META At present, external contributions are not accepted\nInternal Databricks employees are welcome to contribute but are currently limited to tests and supporting refactoring until sufficient tests are in place.",
    "description": "Contributing to DLT-META At present, external contributions are not accepted\nInternal Databricks employees are welcome to contribute but are currently limited to tests and supporting refactoring until sufficient tests are in place.",
    "tags": [],
    "title": "Contributing",
    "uri": "/contributing/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "v0.0.9 Enhancement Added apply_changes_from_snapshot api support in bronze layer: PR Added dlt append_flow api support for silver layer: PR Added support for file metadata columns for autoloader: PR Added support for Bring your own custom transformation: Issue Added support to Unify PyPI releases with GitHub OIDC: PR Added demo for append_flow and file_metadata options: PR Added Demo for silver fanout architecture: PR Added hugo-theme-relearn themee: PR Added unit tests to showcase silver layer fanout examples: PR Added liquid cluster support: PR Added support for UC Volume + Serverless support for CLI, Integration tests and Demos: PR Added Chaining bronze/silver pipelines into single DLT: PR Updates Fixed issue for No such file or directory: ‘/demo’ :PR Fixed issue DLT-META CLI onboard command issue for Azure: databricks.sdk.errors.platform.ResourceAlreadyExists :PR Fixed issue Changed dbfs.create to mkdirs for CLI: PR Fixed issue DLT-META CLI should use pypi lib instead of whl : PR Fixed issue Onboarding with multiple partition columns errors out: PR v0.0.8 Enhancement Added dlt append_flow api support: PR Added dlt append_flow api support for silver layer: PR Added support for file metadata columns for autoloader: PR Added support for Bring your own custom transformation: Issue Added support to Unify PyPI releases with GitHub OIDC: PR Added demo for append_flow and file_metadata options: PR Added Demo for silver fanout architecture: PR Added documentation in docs site for new features: PR Added unit tests to showcase silver layer fanout examples: PR Updates Fixed issue for No such file or directory: ‘/demo’ :PR Fixed issue DLT-META CLI onboard command issue for Azure: databricks.sdk.errors.platform.ResourceAlreadyExists :PR Fixed issue Changed dbfs.create to mkdirs for CLI: PR Fixed issue DLT-META CLI should use pypi lib instead of whl : PR v0.0.7 Enhancement 1. Mismatched Keys: Update read_dlt_delta() with key “source_database” instead of “database” #33 2. Create dlt-meta cli documentation #45 Readme and docs to include above features v0.0.6 Enhancement 1. Migrate to create_streaming_table api from create_streaming_live_table #37 Updates Readme and docs to include above features Added Data Quality support for silver layer Updated existing demos to incorporate unity catalog and integration test framework integration tests framework which can be used to launch demos v0.0.5 New Features 1. Unity Catalog Support (#28) 2. Databricks Labs CLI Support (#28) Added two commands for DLT-META onboard: Captures all onboarding details from command line and launch onboarding job to your databricks workspace deploy: Captures all DLT pipeline details from command line and launch DLT pipeline to your databricks workspace Updates Readme and docs to include above features Updated existing demos to incorporate unity catalog and integration test framework integration tests framework which can be used to launch demos v0.0.4 Bug Fixes Introduced new source detail option for eventhub: eventhub.accessKeySecretName v0.0.3 Bug Fixes Infer datatypes from sequence_by to __START_AT, __END_AT for apply changes API Removed Git release tag from github actions v0.0.2 New Features Table properties support for bronze, quarantine and silver tables using create_streaming_live_table api call Support for track history column using apply_changes api Support for delta as source Validation for bronze/silver onboarding Bug Fixes Input schema parsing issue in onboarding Updates Readme and docs to include above features v.0.0.1 Release v.0.0.1",
    "description": "v0.0.9 Enhancement Added apply_changes_from_snapshot api support in bronze layer: PR Added dlt append_flow api support for silver layer: PR Added support for file metadata columns for autoloader: PR Added support for Bring your own custom transformation: Issue Added support to Unify PyPI releases with GitHub OIDC: PR Added demo for append_flow and file_metadata options: PR Added Demo for silver fanout architecture: PR Added hugo-theme-relearn themee: PR Added unit tests to showcase silver layer fanout examples: PR Added liquid cluster support: PR Added support for UC Volume + Serverless support for CLI, Integration tests and Demos: PR Added Chaining bronze/silver pipelines into single DLT: PR Updates Fixed issue for No such file or directory: ‘/demo’ :PR Fixed issue DLT-META CLI onboard command issue for Azure: databricks.sdk.errors.platform.ResourceAlreadyExists :PR Fixed issue Changed dbfs.create to mkdirs for CLI: PR Fixed issue DLT-META CLI should use pypi lib instead of whl : PR Fixed issue Onboarding with multiple partition columns errors out: PR v0.0.8 Enhancement Added dlt append_flow api support: PR Added dlt append_flow api support for silver layer: PR Added support for file metadata columns for autoloader: PR Added support for Bring your own custom transformation: Issue Added support to Unify PyPI releases with GitHub OIDC: PR Added demo for append_flow and file_metadata options: PR Added Demo for silver fanout architecture: PR Added documentation in docs site for new features: PR Added unit tests to showcase silver layer fanout examples: PR Updates Fixed issue for No such file or directory: ‘/demo’ :PR Fixed issue DLT-META CLI onboard command issue for Azure: databricks.sdk.errors.platform.ResourceAlreadyExists :PR Fixed issue Changed dbfs.create to mkdirs for CLI: PR Fixed issue DLT-META CLI should use pypi lib instead of whl : PR v0.0.7 Enhancement 1. Mismatched Keys: Update read_dlt_delta() with key “source_database” instead of “database” #33 2. Create dlt-meta cli documentation #45 Readme and docs to include above features v0.0.6 Enhancement 1. Migrate to create_streaming_table api from create_streaming_live_table #37 Updates Readme and docs to include above features Added Data Quality support for silver layer Updated existing demos to incorporate unity catalog and integration test framework integration tests framework which can be used to launch demos v0.0.5 New Features 1. Unity Catalog Support (#28) 2. Databricks Labs CLI Support (#28) Added two commands for DLT-META onboard: Captures all onboarding details from command line and launch onboarding job to your databricks workspace deploy: Captures all DLT pipeline details from command line and launch DLT pipeline to your databricks workspace Updates Readme and docs to include above features Updated existing demos to incorporate unity catalog and integration test framework integration tests framework which can be used to launch demos v0.0.4 Bug Fixes Introduced new source detail option for eventhub: eventhub.accessKeySecretName v0.0.3 Bug Fixes Infer datatypes from sequence_by to __START_AT, __END_AT for apply changes API Removed Git release tag from github actions v0.0.2 New Features Table properties support for bronze, quarantine and silver tables using create_streaming_live_table api call Support for track history column using apply_changes api Support for delta as source Validation for bronze/silver onboarding Bug Fixes Input schema parsing issue in onboarding Updates Readme and docs to include above features v.0.0.1 Release v.0.0.1",
    "tags": [],
    "title": "Releases",
    "uri": "/releases/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Project Overview DLT-META is a metadata-driven framework designed to work with Databricks Delta Live Tables (DLT). This framework enables the automation of bronze and silver data pipelines by leveraging metadata recorded in an onboarding JSON file. This file, known as the Dataflowspec, serves as the data flow specification, detailing the source and target metadata required for the pipelines.\nIn practice, a single generic DLT pipeline reads the Dataflowspec and uses it to orchestrate and run the necessary data processing workloads. This approach streamlines the development and management of data pipelines, allowing for a more efficient and scalable data processing workflow\nDLT-META components: Metadata Interface Capture input/output metadata in onboarding file Capture Data Quality Rules Capture processing logic as sql in Silver transformation file Generic DLT pipeline Apply appropriate readers based on input metadata Apply data quality rules with DLT expectations Apply CDC apply changes if specified in metadata Builds DLT graph based on input/output metadata Launch DLT pipeline High-Level Solution overview: How does DLT-META work? Metadata Preparation\nOnboarding Job\nOption#1: DLT-META CLI Option#2: Manual Job option#3: Databricks Notebook Dataflow DLT Pipeline\nOption#1: DLT-META CLI Option#2: DLT-META MANUAL DLT-META DLT Features support Features DLT-META Support Input data sources Autoloader, Delta, Eventhub, Kafka Medallion architecture layers Bronze, Silver Custom transformations Bronze, Silver layer accepts custom functions Data Quality Expecations Support Bronze, Silver layer Quarantine table support Bronze layer apply_changes API support Bronze, Silver layer apply_changes_from_snapshot API support Bronze layer Liquid cluster support Bronze, Bronze Quarantine, Silver tables DLT-META CLI databricks labs dlt-meta onboard, databricks labs dlt-meta deploy Bronze and Silver pipeline chaining Deploy dlt-meta pipeline with layer=bronze_silver option using Direct publishing mode How much does it cost ? DLT-META does not have any direct cost associated with it other than the cost to run the Databricks Delta Live Tables on your environment.The overall cost will be determined primarily by the [Databricks Delta Live Tables Pricing] (https://databricks.com/product/delta-live-tables-pricing-azure)\nMore questions Refer to the FAQ\nGetting Started Refer to the Getting Started guide\nProject Support Please note that all projects in the databrickslabs github account are provided for your exploration only, and are not formally supported by Databricks with Service Level Agreements (SLAs). They are provided AS-IS and we do not make any guarantees of any kind. Please do not submit a support ticket relating to any issues arising from the use of these projects.\nAny issues discovered through the use of this project should be filed as GitHub Issues on the Repo. They will be reviewed as time permits, but there are no formal SLAs for support.\nContributing See our CONTRIBUTING for more details.",
    "description": "Project Overview DLT-META is a metadata-driven framework designed to work with Databricks Delta Live Tables (DLT). This framework enables the automation of bronze and silver data pipelines by leveraging metadata recorded in an onboarding JSON file. This file, known as the Dataflowspec, serves as the data flow specification, detailing the source and target metadata required for the pipelines.\nIn practice, a single generic DLT pipeline reads the Dataflowspec and uses it to orchestrate and run the necessary data processing workloads. This approach streamlines the development and management of data pipelines, allowing for a more efficient and scalable data processing workflow",
    "tags": [],
    "title": "DLT-META",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
