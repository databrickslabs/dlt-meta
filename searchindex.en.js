var relearn_searchindex = [
  {
    "breadcrumb": "DLT-META \u003e Contributing",
    "content": "This document aims to provide complete information needed for anyone who would like to contribute to the dlt-meta project. Your contributions are vital to its success, whether you’re fixing bugs, improving documentation, or adding new features.\nSteps Step 0 - Read the documentation Refer documentation wiki page here that will guide you to access different DLT-META resources like documentation, github repo, presentation etc. Read the getting started link here to understand pre-requisite , setup steps and configuration details.\nPrerequisite\nInstall Databricks CLI to you local machine Authenticate you current machine to a Databricks Workspace Python 3.8.0+ Step 1 - Fork the Repository In case you may not be able to fork this repo because the repository is outside of your enterprise Databricks(EMU) , follow step3 or Fork using a personal github account.\nStep 2 - Clone the Repository Locally Run command “git clone https://github.com/databrickslabs/dlt-meta.git” it will create folder name “dlt-meta” Step 3 - Set Up the Development Environment cd dlt-meta Create python virtual environment python -m venv .venv or python3 -m venv .venv Activate python virtual environment source .venv/bin/activate Install databricks sdk pip install databricks-sdk Install code editor like VS code or any other. Import project into VS code File \u003e Open folder \u003e select above dlt-meta folder from your system Install setuptools and wheel if not already installed pip install setuptools wheel Install the project dependencies specified in setup.py pip install -e . Build the project\n* python setup.py sdist bdist_wheel Install additional dependencies pip install pyspark pip install delta-spark Pip install pytest Step 4 - Running Unit and Integration Tests Unit test are at tests folder\nTo run the test cases, use the pytest command in the terminal To run all tests run - pytest To run specific test- pytest -k “test_case_name” Integration Tests are at integration_tests folder\nTo run integration test run file run_integration_tests.py with mandatory required argument as below e.g. run_integration_tests.py --uc_catalog_name datta_demo --cloud_provider_name aws --dbr_version 14.3 --source cloudfiles --dbfs_path “dbfs:/tmp/DLT-META/” --profile DEFAULT Step 5 - Find Beginner-Friendly Issues Refer open issues [here](https://github.com/databrickslabs/dlt-meta/issues). Step 7 - Work on the Issue To work on the issue Fork the repository as shown below\nHere’s how to fork a repository on GitHub: Go to the repository’s page Click the Fork button A forked copy will be added to your GitHub repositories list A small text below the repository name will confirm that it’s a fork\nYou can also fork a repository on GitHub Desktop: Click Clone Repository in the File menu Select the local directory to clone the repository into Click Continue Comments the proposed sketch design which includes points problem statement, goals and objectives, proposed sketch, scope, implementation plan, risk and mitigation strategies and testing and validation plan. Write the code. Write the unit tests and validate them. Run and validate the integration test cases. Upon successful completion of testing, commit the changes to your git repository Step 8 - Submit a PR Once you have successfully made the changes, commit them to your repository and create pull requests. For more details, refer to the [documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork). Step 9 - Celebrate your Contribution Congratulations.",
    "description": "This document aims to provide complete information needed for anyone who would like to contribute to the dlt-meta project. Your contributions are vital to its success, whether you’re fixing bugs, improving documentation, or adding new features.\nSteps Step 0 - Read the documentation Refer documentation wiki page here that will guide you to access different DLT-META resources like documentation, github repo, presentation etc. Read the getting started link here to understand pre-requisite , setup steps and configuration details.",
    "tags": [],
    "title": "New Contributor Onboarding Guide",
    "uri": "/contributing/onboarding/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "The following tutorial will guide you through the process for setting up the DLT-META on your Databricks Lakehouse environment.\nYou will deploy/configure the solution, configure a database/table for bronze and silver layer as per below stages.",
    "description": "The following tutorial will guide you through the process for setting up the DLT-META on your Databricks Lakehouse environment.\nYou will deploy/configure the solution, configure a database/table for bronze and silver layer as per below stages.",
    "tags": [],
    "title": "Getting Started",
    "uri": "/getting_started/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Getting Started",
    "content": "Directory structure conf/ onboarding.json silver_transformations.json dqe/ bronze_data_quality_expectations.json Create onboarding.json Create silver_transformations.json Create data quality rules json’s for each entity e.g. Data Quality Rules The onboarding.json file contains links to silver_transformations.json and data quality expectation files dqe.\nonboarding.json File structure: Examples( Autoloader, Eventhub, Kafka ) env is your environment placeholder e.g dev, prod, stag\nField Description data_flow_id This is unique identifier for pipeline data_flow_group This is group identifier for launching multiple pipelines under single Lakeflow Declarative Pipeline source_format Source format e.g cloudFiles, eventhub, kafka, delta, snapshot source_details This map Type captures all source details for cloudfiles = source_schema_path, source_path_{env}, source_catalog, source_database, source_metadata For eventhub= source_schema_path , eventhub.accessKeyName, eventhub.accessKeySecretName, eventhub.name , eventhub.secretsScopeName , kafka.sasl.mechanism, kafka.security.protocol, eventhub.namespace, eventhub.port. For Source schema file spark DDL schema format parsing is supported In case of custom schema format then write schema parsing function bronze_schema_mapper(schema_file_path, spark):Schema and provide to OnboardDataflowspec initialization e.g onboardDataFlowSpecs = OnboardDataflowspec(spark, dict_obj,bronze_schema_mapper).onboardDataFlowSpecs(). For cloudFiles option _metadata columns addtiion there is source_metadata tag with attributes: include_autoloader_metadata_column flag (True or False value) will add _metadata column to target bronze dataframe, autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata,select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column. for snapshot= snapshot_format, source_path_{env} bronze_catalog_{env} Unity catalog name bronze_database_{env} Delta lake bronze database name. bronze_table Delta lake bronze table name bronze_table_comment Bronze table comment bronze_reader_options Reader options which can be provided to spark reader e.g multiline=true,header=true in json format bronze_parition_columns Bronze table partition cols list bronze_cluster_by Bronze tables cluster by cols list bronze_cdc_apply_changes Bronze cdc apply changes Json bronze_apply_changes_from_snapshot Bronze apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] bronze_table_path_{env} Bronze table storage path. bronze_table_properties Lakeflow Declarative Pipeline table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\" } bronze_sink Lakeflow Declarative Pipeline Sink API properties: e.g Delta: {\"name\": \"bronze_sink\",\"format\": \"delta\",\"options\": {\"tableName\": \"my_catalog.my_schema.my_table\"}}, Kafka:{\"name\": \"bronze_sink\",\"format\": \"kafka\",\"options\": { \"kafka.bootstrap.servers\": \"host:port\",\"subscribe\": \"my_topic\"}} bronze_data_quality_expectations_json Bronze table data quality expectations bronze_catalog_quarantine_{env} Unity catalog name bronze_database_quarantine_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table Bronze Table for quarantine data which fails expectations bronze_quarantine_table_comment Bronze quarantine table comment bronze_quarantine_table_path_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table_partitions Bronze quarantine tables partition cols bronze_quarantine_table_cluster_by Bronze quarantine tables cluster cols bronze_quarantine_table_properties Lakeflow Declarative Pipeline table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\" } bronze_append_flows Bronze table append flows json. e.g.\"bronze_append_flows\":[{\"name\":\"customer_bronze_flow\", \"create_streaming_table\": false,\"source_format\": \"cloudFiles\", \"source_details\": {\"source_database\": \"APP\",\"source_table\":\"CUSTOMERS\", \"source_path_dev\": \"tests/resources/data/customers\", \"source_schema_path\": \"tests/resources/schema/customer_schema.ddl\"},\"reader_options\": {\"cloudFiles.format\": \"json\",\"cloudFiles.inferColumnTypes\": \"true\",\"cloudFiles.rescuedDataColumn\": \"_rescued_data\"},\"once\": true}] silver_catalog_{env} Unit Catalog name. silver_database_{env} Silver database name. silver_table Silver table name silver_table_comment Silver table comments silver_partition_columns Silver table partition columns list silver_cluster_by Silver tables cluster by cols list silver_cdc_apply_changes Silver cdc apply changes Json silver_table_path_{env} Silver table storage path. silver_table_properties Lakeflow Declarative Pipeline table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\"} silver_sink Lakeflow Declarative Pipeline Sink API properties: e.g Delta:{\"name\": \"silver_sink\",\"format\": \"delta\",\"options\": {\"tableName\": \"my_catalog.my_schema.my_table\"}}, Kafka:{\"name\": \"silver_sink\",\"format\": \"kafka\",\"options\": { \"kafka.bootstrap.servers\": \"host:port\",\"subscribe\": \"my_topic\"}} silver_transformation_json Silver table sql transformation json path silver_data_quality_expectations_json_{env} Silver table data quality expectations json file path silver_append_flows Silver table append flows json. e.g.`“silver_append_flows”:[{“name”:“customer_bronze_flow”, silver_apply_changes_from_snapshot Silver apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] Data Quality Rules File Structure(Examples) Field Description expect Specify multiple data quality sql for each field when records that fail validation should be included in the target dataset expect_or_fail Specify multiple data quality sql for each field when records that fail validation should halt pipeline execution expect_or_drop Specify multiple data quality sql for each field when records that fail validation should be dropped from the target dataset expect_or_quarantine Specify multiple data quality sql for each field when records that fails validation will be dropped from main table and inserted into quarantine table specified in dataflowspec (only applicable for Bronze layer) Silver transformation File Structure(Example) Field Description target_table Specify target table name : Type String target_partition_cols Specify partition columns : Type Array select_exp Specify SQL expressions : Type Array where_clause Specify filter conditions if you want to prevent certain records from main input : Type Array",
    "description": "Directory structure conf/ onboarding.json silver_transformations.json dqe/ bronze_data_quality_expectations.json Create onboarding.json Create silver_transformations.json Create data quality rules json’s for each entity e.g. Data Quality Rules The onboarding.json file contains links to silver_transformations.json and data quality expectation files dqe.\nonboarding.json File structure: Examples( Autoloader, Eventhub, Kafka ) env is your environment placeholder e.g dev, prod, stag\nField Description data_flow_id This is unique identifier for pipeline data_flow_group This is group identifier for launching multiple pipelines under single Lakeflow Declarative Pipeline source_format Source format e.g cloudFiles, eventhub, kafka, delta, snapshot source_details This map Type captures all source details for cloudfiles = source_schema_path, source_path_{env}, source_catalog, source_database, source_metadata For eventhub= source_schema_path , eventhub.accessKeyName, eventhub.accessKeySecretName, eventhub.name , eventhub.secretsScopeName , kafka.sasl.mechanism, kafka.security.protocol, eventhub.namespace, eventhub.port. For Source schema file spark DDL schema format parsing is supported In case of custom schema format then write schema parsing function bronze_schema_mapper(schema_file_path, spark):Schema and provide to OnboardDataflowspec initialization e.g onboardDataFlowSpecs = OnboardDataflowspec(spark, dict_obj,bronze_schema_mapper).onboardDataFlowSpecs(). For cloudFiles option _metadata columns addtiion there is source_metadata tag with attributes: include_autoloader_metadata_column flag (True or False value) will add _metadata column to target bronze dataframe, autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata,select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column. for snapshot= snapshot_format, source_path_{env} bronze_catalog_{env} Unity catalog name bronze_database_{env} Delta lake bronze database name. bronze_table Delta lake bronze table name bronze_table_comment Bronze table comment bronze_reader_options Reader options which can be provided to spark reader e.g multiline=true,header=true in json format bronze_parition_columns Bronze table partition cols list bronze_cluster_by Bronze tables cluster by cols list bronze_cdc_apply_changes Bronze cdc apply changes Json bronze_apply_changes_from_snapshot Bronze apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] bronze_table_path_{env} Bronze table storage path. bronze_table_properties Lakeflow Declarative Pipeline table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\" } bronze_sink Lakeflow Declarative Pipeline Sink API properties: e.g Delta: {\"name\": \"bronze_sink\",\"format\": \"delta\",\"options\": {\"tableName\": \"my_catalog.my_schema.my_table\"}}, Kafka:{\"name\": \"bronze_sink\",\"format\": \"kafka\",\"options\": { \"kafka.bootstrap.servers\": \"host:port\",\"subscribe\": \"my_topic\"}} bronze_data_quality_expectations_json Bronze table data quality expectations bronze_catalog_quarantine_{env} Unity catalog name bronze_database_quarantine_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table Bronze Table for quarantine data which fails expectations bronze_quarantine_table_comment Bronze quarantine table comment bronze_quarantine_table_path_{env} Bronze database for quarantine data which fails expectations. bronze_quarantine_table_partitions Bronze quarantine tables partition cols bronze_quarantine_table_cluster_by Bronze quarantine tables cluster cols bronze_quarantine_table_properties Lakeflow Declarative Pipeline table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\" } bronze_append_flows Bronze table append flows json. e.g.\"bronze_append_flows\":[{\"name\":\"customer_bronze_flow\", \"create_streaming_table\": false,\"source_format\": \"cloudFiles\", \"source_details\": {\"source_database\": \"APP\",\"source_table\":\"CUSTOMERS\", \"source_path_dev\": \"tests/resources/data/customers\", \"source_schema_path\": \"tests/resources/schema/customer_schema.ddl\"},\"reader_options\": {\"cloudFiles.format\": \"json\",\"cloudFiles.inferColumnTypes\": \"true\",\"cloudFiles.rescuedDataColumn\": \"_rescued_data\"},\"once\": true}] silver_catalog_{env} Unit Catalog name. silver_database_{env} Silver database name. silver_table Silver table name silver_table_comment Silver table comments silver_partition_columns Silver table partition columns list silver_cluster_by Silver tables cluster by cols list silver_cdc_apply_changes Silver cdc apply changes Json silver_table_path_{env} Silver table storage path. silver_table_properties Lakeflow Declarative Pipeline table properties map. e.g. {\"pipelines.autoOptimize.managed\": \"false\" , \"pipelines.autoOptimize.zOrderCols\": \"year,month\", \"pipelines.reset.allowed\": \"false\"} silver_sink Lakeflow Declarative Pipeline Sink API properties: e.g Delta:{\"name\": \"silver_sink\",\"format\": \"delta\",\"options\": {\"tableName\": \"my_catalog.my_schema.my_table\"}}, Kafka:{\"name\": \"silver_sink\",\"format\": \"kafka\",\"options\": { \"kafka.bootstrap.servers\": \"host:port\",\"subscribe\": \"my_topic\"}} silver_transformation_json Silver table sql transformation json path silver_data_quality_expectations_json_{env} Silver table data quality expectations json file path silver_append_flows Silver table append flows json. e.g.`“silver_append_flows”:[{“name”:“customer_bronze_flow”, silver_apply_changes_from_snapshot Silver apply changes from snapshot Json e.g. Mandatory fields: keys=[“userId”], scd_type=1 or 2 optional fields: track_history_column_list=[col1], track_history_except_column_list=[col2] Data Quality Rules File Structure(Examples) Field Description expect Specify multiple data quality sql for each field when records that fail validation should be included in the target dataset expect_or_fail Specify multiple data quality sql for each field when records that fail validation should halt pipeline execution expect_or_drop Specify multiple data quality sql for each field when records that fail validation should be dropped from the target dataset expect_or_quarantine Specify multiple data quality sql for each field when records that fails validation will be dropped from main table and inserted into quarantine table specified in dataflowspec (only applicable for Bronze layer) Silver transformation File Structure(Example) Field Description target_table Specify target table name : Type String target_partition_cols Specify partition columns : Type Array select_exp Specify SQL expressions : Type Array where_clause Specify filter conditions if you want to prevent certain records from main input : Type Array",
    "tags": [],
    "title": "Metadata Preparation",
    "uri": "/getting_started/metadatapreperation/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Getting Started",
    "content": "Prerequisites: Python 3.8.0 + Databricks CLI Steps: Install and authenticate Databricks CLI:\ndatabricks auth login --host WORKSPACE_HOST Install dlt-meta via Databricks CLI:\ndatabricks labs install dlt-meta Clone dlt-meta repository:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:\ncd dlt-meta Create Python virtual environment:\npython -m venv .venv Activate virtual environment:\nsource .venv/bin/activate Install required packages:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 # Integration test requirements pip install \"typer[all]==0.6.1\" Set environment variables:\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home OnboardJob Run Onboarding using dlt-meta cli command: databricks labs dlt-meta onboard The command will prompt you to provide onboarding details. If you have cloned the dlt-meta repository, you can accept the default values which will use the configuration from the demo folder. Above onboard cli command will:\nPush code and data to your Databricks workspace Create an onboarding job Display a success message: Job created successfully. job_id={job_id}, url=https://{databricks workspace url}/jobs/{job_id} Job URL will automatically open in your default browser. Once onboarding jobs is finished deploy bronze and silver Lakeflow Declarative Pipeline using below command\nDLT-META Lakeflow Declarative Pipeline: Deploy Bronze and Silver layer into single pipeline databricks labs dlt-meta deploy Above command will prompt you to provide pipeline details. Please provide respective details for schema which you provided in above steps Above deploy cli command will: Deploy Lakeflow Declarative pipeline with dlt-meta configuration like layer, group, dataflowSpec table details etc to your databricks workspace Display message: dlt-meta pipeline={pipeline_id} created and launched with update_id={pipeline_update_id}, url=https://{databricks workspace url}/#joblist/pipelines/{pipeline_id} Pipline URL will automatically open in your defaul browser.",
    "description": "Prerequisites: Python 3.8.0 + Databricks CLI Steps: Install and authenticate Databricks CLI:\ndatabricks auth login --host WORKSPACE_HOST Install dlt-meta via Databricks CLI:\ndatabricks labs install dlt-meta Clone dlt-meta repository:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:\ncd dlt-meta Create Python virtual environment:\npython -m venv .venv Activate virtual environment:\nsource .venv/bin/activate Install required packages:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 # Integration test requirements pip install \"typer[all]==0.6.1\" Set environment variables:",
    "tags": [],
    "title": "DLT-META CLI",
    "uri": "/getting_started/dltmeta_cli/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Getting Started",
    "content": "OnboardJob Option#1: using Databricks Python whl job Go to your Databricks landing page and do one of the following:\nIn the sidebar, click Jobs Icon Workflows and click Create Job Button.\nIn the sidebar, click New Icon New and select Job from the menu.\nIn the task dialog box that appears on the Tasks tab, replace Add a name for your job… with your job name, for example, Python wheel example.\nIn Task name, enter a name for the task, for example, dlt_meta_onboarding_pythonwheel_task.\nIn Type, select Python wheel.\nIn Package name, enter dlt_meta.\nIn Entry point, enter run.\nClick Add under Dependent Libraries. In the Add dependent library dialog, under Library Type, click PyPI. Enter Package = dlt-meta\nClick Add.\nIn Parameters, select keyword argument then select JSON. Past below json parameters with :\nWithout Unity Cataglog { \"onboard_layer\": \"bronze_silver\", \"database\": \"dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"silver_dataflowspec_path\": \"dbfs:/onboarding_tables_cdc/silver\", \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"import_author\": \"Ravi\", \"version\": \"v1\", \"bronze_dataflowspec_path\": \"dbfs:/onboarding_tables_cdc/bronze\", \"onboard_layer\": \"bronze_silver\", \"uc_enabled\": \"False\", \"overwrite\": \"True\", \"env\": \"dev\" } with Unity catalog { \"onboard_layer\": \"bronze_silver\", \"database\": \"uc_name.dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"import_author\": \"Ravi\", \"version\": \"v1\", \"uc_enabled\": \"True\", \"overwrite\": \"True\", \"env\": \"dev\" } Note in database field you need to provide catalog name then schema name as \u003c\u003cuc_name\u003e\u003e.\u003c\u003cschema\u003e\u003e Alternatly you can enter keyword arguments, click + Add and enter a key and value. Click + Add again to enter more arguments.\nClick Save task.\nRun now\nMake sure job run successfully. Verify metadata in your dataflow spec tables entered in step: 11 e.g dlt_demo.bronze_dataflowspec_table , dlt_demo.silver_dataflowspec_table\nOption#2: Databricks Notebook Copy below code to databricks notebook cells %pip install dlt-meta without unity catalog onboarding_params_map = { \"database\": \"dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"bronze_dataflowspec_path\": \"dbfs:/onboarding_tables_cdc/bronze\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"silver_dataflowspec_path\": \"dbfs:/onboarding_tables_cdc/silver\", \"overwrite\": \"True\", \"env\": \"dev\", \"version\": \"v1\", \"import_author\": \"Ravi\" } from src.onboard_dataflowspec import OnboardDataflowspec OnboardDataflowspec(spark, onboarding_params_map).onboard_dataflow_specs() with unity catalog onboarding_params_map = { \"database\": \"uc_name.dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\",, \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"overwrite\": \"True\", \"env\": \"dev\", \"version\": \"v1\", \"import_author\": \"Ravi\" } from src.onboard_dataflowspec import OnboardDataflowspec OnboardDataflowspec(spark, onboarding_params_map, uc_enabled=True).onboard_dataflow_specs() Specify your onboarding config params in above onboarding_params_map\nRun notebook cells\nLakeflow Declarative Pipeline: Lakeflow Declarative Pipelines launch notebook Go to your Databricks landing page and select Create a notebook, or click New Icon New in the sidebar and select Notebook. The Create Notebook dialog appears.\nIn the Create Notebook dialogue, give your notebook a name e.g dlt_meta_pipeline and select Python from the Default Language dropdown menu. You can leave Cluster set to the default value. The Lakeflow Declarative Pipelines runtime creates a cluster before it runs your pipeline.\nClick Create.\nYou can add the example dlt pipeline code or import iPython notebook as is.\n%pip install dlt-meta layer = spark.conf.get(\"layer\", None) from src.dataflow_pipeline import DataflowPipeline DataflowPipeline.invoke_dlt_pipeline(spark, layer) Create Bronze Lakeflow Declarative Pipeline Click Jobs Icon Workflows in the sidebar, click the Lakeflow Declarative Pipelines tab, and click Create Pipeline.\nGive the pipeline a name e.g. DLT_META_BRONZE and click File Picker Icon to select a notebook dlt_meta_pipeline created in step: Create a dlt launch notebook.\nOptionally enter a storage location for output data from the pipeline. The system uses a default location if you leave Storage location empty.\nSelect Triggered for Pipeline Mode.\nEnter Configuration parameters e.g.\n\"layer\": \"bronze\", \"bronze.dataflowspecTable\": \"dataflowspec table name\", \"bronze.group\": \"enter group name from metadata e.g. G1\", Enter target schema where you wants your bronze tables to be created\nClick Create.\nStart pipeline: click the Start button on in top panel. The system returns a message confirming that your pipeline is starting\nCreate Silver Lakeflow Declarative Pipelines Click Jobs Icon Workflows in the sidebar, click the Lakeflow Declarative Pipelines tab, and click Create Pipeline.\nGive the pipeline a name e.g. DLT_META_SILVER and click File Picker Icon to select a notebook dlt_meta_pipeline created in step: Create a dlt launch notebook.\nOptionally enter a storage location for output data from the pipeline. The system uses a default location if you leave Storage location empty.\nSelect Triggered for Pipeline Mode.\nEnter Configuration parameters e.g.\n\"layer\": \"silver\", \"silver.dataflowspecTable\": \"dataflowspec table name\", \"silver.group\": \"enter group name from metadata e.g. G1\", Enter target schema where you wants your silver tables to be created\nClick Create.\nStart pipeline: click the Start button on in top panel. The system returns a message confirming that your pipeline is starting",
    "description": "OnboardJob Option#1: using Databricks Python whl job Go to your Databricks landing page and do one of the following:\nIn the sidebar, click Jobs Icon Workflows and click Create Job Button.\nIn the sidebar, click New Icon New and select Job from the menu.\nIn the task dialog box that appears on the Tasks tab, replace Add a name for your job… with your job name, for example, Python wheel example.",
    "tags": [],
    "title": "DLT-META Manual",
    "uri": "/getting_started/dltmeta_manual/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Getting Started",
    "content": "Prerequisites System Requirements Python 3.8.0 or higher Databricks CLI (latest version, e.g., 0.244.0) Configured workspace access Initial Setup Authenticate with Databricks:\ndatabricks auth login --host WORKSPACE_HOST Setup Python Environment:\ngit clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk Deployment Options Deploy to Databricks Create Custom App:\ndatabricks apps create demo-dltmeta Note: Wait for command completion (a few minutes)\nSetup App Code:\ncd dlt-meta/lakehouse_app # Replace testapp with your preferred folder name databricks sync . /Workspace/Users/\u003cuser1.user2\u003e@databricks.com/testapp # Deploy the app databricks apps deploy demo-dltmeta --source-code-path /Workspace/Users/\u003cuser1.user2\u003e@databricks.com/testapp Access the App:\nOpen URL from step 1 log, or Navigate: Databricks Web UI → New → App → Back to App → Search your app name Run Locally Setup Environment:\ncd dlt-meta/lakehouse_app pip install -r requirements.txt Configure Databricks:\ndatabricks configure --host \u003cyour databricks host url\u003e --token \u003cyour token\u003e Start App:\npython App.py Access at: http://127.0.0.1:5000\nUsing DLT-META App App User Setup The app creates a dedicated user account that:\nHandles onboarding, deployment, and demo execution Requires specific permissions for UC catalogs and schemas Example username format: “app-40zbx9_demo-dltmeta” Getting Started Initial Setup:\nLaunch app in browser Click “Setup dlt-meta project environment” This initializes the environment for onboarding and deployment Pipeline Management:\nUse “UI” tab to onboard and deploy pipelines Configure pipelines according to your requirements Onboarding Pipeline: Pipeline onboarding interface for configuring new data pipelines\nDeploying Pipeline: Pipeline deployment interface for managing and deploying pipelines\nDemo Access:\nAvailable demos can be found under “Demo” tab Run pre-configured demo pipelines to explore features Demo interface showing available example pipelines\nCommand Line Interface:\nAccess CLI features under the “CLI” tab Execute commands directly from the web interface CLI interface for command-line operations",
    "description": "Prerequisites System Requirements Python 3.8.0 or higher Databricks CLI (latest version, e.g., 0.244.0) Configured workspace access Initial Setup Authenticate with Databricks:\ndatabricks auth login --host WORKSPACE_HOST Setup Python Environment:\ngit clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk Deployment Options Deploy to Databricks Create Custom App:\ndatabricks apps create demo-dltmeta Note: Wait for command completion (a few minutes)\nSetup App Code:",
    "tags": [],
    "title": "DLT-META Lakehouse App",
    "uri": "/getting_started/app/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "DAIS 2023 DEMO: Showcases DLT-META’s capabilities of creating Bronze and Silver Lakeflow Declarative Pipelines with initial and incremental mode automatically. Databricks Techsummit Demo: 100s of data sources ingestion in bronze and silver Lakeflow Declarative Pipelines automatically. Append FLOW Autoloader Demo: Write to same target from multiple sources using append_flow and adding file metadata using File metadata column Append FLOW Eventhub Demo: Write to same target from multiple sources using append_flow and adding using File metadata column Silver Fanout Demo: This demo will showcase fanout architecture can be implemented in silver layer Apply Changes From Snapshot Demo: This demo will showcase create_auto_cdc_from_snapshot_flow can be implemented inside bronze and silver layer Lakeflow Declarative Pipelines Sink Demo: This demo showcases the implementation of write to external sinks like delta and kafka DAB Demo: This demo showcases how to use Databricks Assets Bundles with dlt-meta",
    "description": "DAIS 2023 DEMO: Showcases DLT-META’s capabilities of creating Bronze and Silver Lakeflow Declarative Pipelines with initial and incremental mode automatically. Databricks Techsummit Demo: 100s of data sources ingestion in bronze and silver Lakeflow Declarative Pipelines automatically. Append FLOW Autoloader Demo: Write to same target from multiple sources using append_flow and adding file metadata using File metadata column Append FLOW Eventhub Demo: Write to same target from multiple sources using append_flow and adding using File metadata column Silver Fanout Demo: This demo will showcase fanout architecture can be implemented in silver layer Apply Changes From Snapshot Demo: This demo will showcase create_auto_cdc_from_snapshot_flow can be implemented inside bronze and silver layer Lakeflow Declarative Pipelines Sink Demo: This demo showcases the implementation of write to external sinks like delta and kafka DAB Demo: This demo showcases how to use Databricks Assets Bundles with dlt-meta",
    "tags": [],
    "title": "Demo",
    "uri": "/demo/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "DAIS 2023 DEMO: DAIS 2023 Session Recording This demo showcases DLT-META’s capabilities of creating Bronze and Silver Lakeflow Declarative pipeline with initial and incremental mode automatically.\nCustomer and Transactions feeds for initial load Adds new feeds Product and Stores to existing Bronze and Silver Lakeflow Declarative pipeline with metadata changes. Runs Bronze and Silver Lakeflow Declarative Pipeline for incremental load for CDC events Steps to launch DAIS demo in your Databricks workspace: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 Clone dlt-meta:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:\ncd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run the command:\npython demo/launch_dais_demo.py --uc_catalog_name=\u003c\u003cuc catalog name\u003e\u003e --cloud_provider_name=\u003c\u003c\u003e\u003e uc_catalog_name : unit catalog name cloud_provider_name : aws or azure or gcp you can provide --profile=databricks_profile name in case you already have databricks cli otherwise command prompt will ask host and token.",
    "description": "DAIS 2023 DEMO: DAIS 2023 Session Recording This demo showcases DLT-META’s capabilities of creating Bronze and Silver Lakeflow Declarative pipeline with initial and incremental mode automatically.\nCustomer and Transactions feeds for initial load Adds new feeds Product and Stores to existing Bronze and Silver Lakeflow Declarative pipeline with metadata changes. Runs Bronze and Silver Lakeflow Declarative Pipeline for incremental load for CDC events Steps to launch DAIS demo in your Databricks workspace: Launch Command Prompt",
    "tags": [],
    "title": "DAIS DEMO",
    "uri": "/demo/dais/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Databricks Tech Summit FY2024 DEMO: This demo will launch auto generated tables(100s) inside single bronze and silver Lakeflow Declarative Pipeline using dlt-meta.\nLaunch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 Clone dlt-meta:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:\ncd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run the command:\npython demo/launch_techsummit_demo.py --uc_catalog_name=\u003c\u003cUnity Catalog name\u003e\u003e --cloud_provider_name=aws uc_catalog_name : Unity Catalog name cloud_provider_name : aws or azure you can provide --profile=databricks_profile name in case you already have databricks cli otherwise command prompt will ask host and token",
    "description": "Databricks Tech Summit FY2024 DEMO: This demo will launch auto generated tables(100s) inside single bronze and silver Lakeflow Declarative Pipeline using dlt-meta.\nLaunch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 Clone dlt-meta:",
    "tags": [],
    "title": "Tech Summit DEMO",
    "uri": "/demo/techsummit/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Append FLOW Autoloader Demo: This demo will perform following tasks:\nRead from different source paths using autoloader and write to same target using dlt.append_flow API Read from different delta tables and write to same silver table using append_flow API Add file_name and file_path to target bronze table for autoloader source using File metadata column Append flow with autoloader Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 Clone dlt-meta:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:\ncd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run the command:\npython demo/launch_af_cloudfiles_demo.py --cloud_provider_name=aws --dbr_version=15.3.x-scala2.12 --dbfs_path=dbfs:/tmp/DLT-META/demo/ --uc_catalog_name=dlt_meta_uc cloud_provider_name : aws or azure or gcp db_version : Databricks Runtime Version dbfs_path : Path on your Databricks workspace where demo will be copied for launching DLT-META Pipelines uc_catalog_name: Unity catalog name you can provide --profile=databricks_profile name in case you already have databricks cli otherwise command prompt will ask host and token",
    "description": "Append FLOW Autoloader Demo: This demo will perform following tasks:\nRead from different source paths using autoloader and write to same target using dlt.append_flow API Read from different delta tables and write to same silver table using append_flow API Add file_name and file_path to target bronze table for autoloader source using File metadata column Append flow with autoloader Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:",
    "tags": [],
    "title": "Append FLOW Autoloader Demo",
    "uri": "/demo/append_flow_cf/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Append FLOW Autoloader Demo: Read from different eventhub topics and write to same target tables using dlt.append_flow API Steps: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 Clone dlt-meta:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:\ncd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Configure Eventhub\nNeeds eventhub instance running\nNeed two eventhub topics first for main feed (eventhub_name) and second for append flow feed (eventhub_name_append_flow)\nCreate databricks secrets scope for eventhub keys\ncommandline databricks secrets create-scope eventhubs_dltmeta_creds databricks secrets put-secret --json '{ \"scope\": \"eventhubs_dltmeta_creds\", \"key\": \"RootManageSharedAccessKey\", \"string_value\": \"\u003c\u003cvalue\u003e\u003e\" }' Create databricks secrets to store producer and consumer keys using the scope created in step 2\nFollowing are the mandatory arguments for running EventHubs demo\ncloud_provider_name: Cloud provider name e.g. aws or azure dbr_version: Databricks Runtime Version e.g. 15.3.x-scala2.12 uc_catalog_name : unity catalog name e.g. dlt_meta_uc dbfs_path: Path on your Databricks workspace where demo will be copied for launching DLT-META Pipelines e.g. dbfs:/tmp/DLT-META/demo/ eventhub_namespace: Eventhub namespace e.g. dltmeta eventhub_name : Primary Eventhubname e.g. dltmeta_demo eventhub_name_append_flow: Secondary eventhub name for appendflow feed e.g. dltmeta_demo_af eventhub_producer_accesskey_name: Producer databricks access keyname e.g. RootManageSharedAccessKey eventhub_consumer_accesskey_name: Consumer databricks access keyname e.g. RootManageSharedAccessKey eventhub_secrets_scope_name: Databricks secret scope name e.g. eventhubs_dltmeta_creds eventhub_port: Eventhub port Run the command: python demo/launch_af_eventhub_demo.py --cloud_provider_name=aws --uc_catalog_name=dlt_meta_uc --eventhub_name=dltmeta_demo --eventhub_name_append_flow=dltmeta_demo_af --eventhub_secrets_scope_name=dltmeta_eventhub_creds --eventhub_namespace=dltmeta --eventhub_port=9093 --eventhub_producer_accesskey_name=RootManageSharedAccessKey --eventhub_consumer_accesskey_name=RootManageSharedAccessKey --eventhub_accesskey_secret_name=RootManageSharedAccessKey",
    "description": "Append FLOW Autoloader Demo: Read from different eventhub topics and write to same target tables using dlt.append_flow API Steps: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 Clone dlt-meta:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:",
    "tags": [],
    "title": "Append FLOW Eventhub Demo",
    "uri": "/demo/append_flow_eh/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Silver Fanout Demo This demo will perform following steps Showcase onboarding process for silver fanout pattern Run onboarding for the bronze cars table, which contains data from various countries. Run onboarding for the silver tables, which have a where_clause based on the country condition in silver_transformations_cars.json. Run Bronze for cars tables Run onboarding for the silver tables, fanning out from the bronze cars tables to country-specific tables such as cars_usa, cars_uk, cars_germany, and cars_japan. Steps: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 Clone dlt-meta:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:\ncd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run the command:\npython demo/launch_silver_fanout_demo.py --uc_catalog_name=\u003c\u003cuc catalog name\u003e\u003e --cloud_provider_name=aws uc_catalog_name : aws or azure cloud_provider_name : aws or azure you can provide --profile=databricks_profile name in case you already have databricks cli otherwise command prompt will ask host and token. a. Databricks Workspace URL: Enter your workspace URL, with the format https://.cloud.databricks.com. To get your workspace URL, see Workspace instance names, URLs, and IDs.\nb. Token: - In your Databricks workspace, click your Databricks username in the top bar, and then select User Settings from the drop down.\n- On the Access tokens tab, click Generate new token. - (Optional) Enter a comment that helps you to identify this token in the future, and change the token’s default lifetime of 90 days. To create a token with no lifetime (not recommended), leave the Lifetime (days) box empty (blank). - Click Generate. - Copy the displayed token - Paste to command prompt",
    "description": "Silver Fanout Demo This demo will perform following steps Showcase onboarding process for silver fanout pattern Run onboarding for the bronze cars table, which contains data from various countries. Run onboarding for the silver tables, which have a where_clause based on the country condition in silver_transformations_cars.json. Run Bronze for cars tables Run onboarding for the silver tables, fanning out from the bronze cars tables to country-specific tables such as cars_usa, cars_uk, cars_germany, and cars_japan. Steps: Launch Command Prompt",
    "tags": [],
    "title": "Silver Fanout Demo",
    "uri": "/demo/silver_fanout/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Apply Changes From Snapshot Demo This demo will perform following steps Showcase onboarding process for apply changes from snapshot pattern Run onboarding for the bronze stores and products tables, which contains data snapshot data in csv files. Run Bronze Lakeflow Declarative Pipeline to load initial snapshot (LOAD_1.csv) Upload incremental snapshot LOAD_2.csv version=2 for stores and product Run Bronze Lakeflow Declarative Pipeline to load incremental snapshot (LOAD_2.csv). Stores is scd_type=2 so updated records will expired and added new records with version_number. Products is scd_type=1 so in case records missing for scd_type=1 will be deleted. Upload incremental snapshot LOAD_3.csv version=3 for stores and product Run Bronze Lakeflow Declarative Pipeline to load incremental snapshot (LOAD_3.csv). Stores is scd_type=2 so updated records will expired and added new records with version_number. Products is scd_type=1 so in case records missing for scd_type=1 will be deleted. Steps: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 Clone dlt-meta:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:\ncd dlt-meta Set python environment variable into terminal\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run the command:\npython demo/launch_acfs_demo.py --uc_catalog_name=\u003c\u003cuc catalog name\u003e\u003e uc_catalog_name : Unity catalog name you can provide --profile=databricks_profile name in case you already have databricks cli otherwise command prompt will ask host and token.",
    "description": "Apply Changes From Snapshot Demo This demo will perform following steps Showcase onboarding process for apply changes from snapshot pattern Run onboarding for the bronze stores and products tables, which contains data snapshot data in csv files. Run Bronze Lakeflow Declarative Pipeline to load initial snapshot (LOAD_1.csv) Upload incremental snapshot LOAD_2.csv version=2 for stores and product Run Bronze Lakeflow Declarative Pipeline to load incremental snapshot (LOAD_2.csv). Stores is scd_type=2 so updated records will expired and added new records with version_number. Products is scd_type=1 so in case records missing for scd_type=1 will be deleted. Upload incremental snapshot LOAD_3.csv version=3 for stores and product Run Bronze Lakeflow Declarative Pipeline to load incremental snapshot (LOAD_3.csv). Stores is scd_type=2 so updated records will expired and added new records with version_number. Products is scd_type=1 so in case records missing for scd_type=1 will be deleted. Steps: Launch Command Prompt",
    "tags": [],
    "title": "Silver Fanout Demo",
    "uri": "/demo/apply_changes_from_snapshot/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "Lakeflow Declarative Pipelines Sink Demo This demo will perform following steps:\nShowcase onboarding process for dlt writing to external sink pattern Run onboarding for the bronze iot events Publish test events to kafka topic Run Bronze Lakeflow Declarative Pipelines which will read from kafka source topic and write to: Events delta table into UC Create quarantine table as per data quality expectations Writes to external kafka topics Writes to external dbfs location as external delta sink Steps: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 Clone dlt-meta:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:\ncd dlt-meta Set python environment variable into terminal:\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Configure Kafka (Optional): If you are using secrets for kafka, create databricks secrets scope for source and sink kafka:\ndatabricks secrets create-scope \u003c\u003cn\u003e\u003e databricks secrets put-secret --json '{ \"scope\": \"\u003c\u003cn\u003e\u003e\", \"key\": \"\u003c\u003ckeyname\u003e\u003e\", \"string_value\": \"\u003c\u003cvalue\u003e\u003e\" }' Run the command:\npython demo/launch_dlt_sink_demo.py --uc_catalog_name=\u003c\u003cuc_catalog_name\u003e\u003e --source=kafka --kafka_source_topic=\u003c\u003ckafka source topic name\u003e\u003e --kafka_sink_topic=\u003c\u003ckafka sink topic name\u003e\u003e --kafka_source_servers_secrets_scope_name=\u003c\u003ckafka source servers secret name\u003e\u003e --kafka_source_servers_secrets_scope_key=\u003c\u003ckafka source server secret scope key name\u003e\u003e --kafka_sink_servers_secret_scope_name=\u003c\u003ckafka sink server secret scope key name\u003e\u003e --kafka_sink_servers_secret_scope_key=\u003c\u003ckafka sink servers secret scope key name\u003e\u003e --profile=\u003c\u003cDEFAULT\u003e\u003e",
    "description": "Lakeflow Declarative Pipelines Sink Demo This demo will perform following steps:\nShowcase onboarding process for dlt writing to external sink pattern Run onboarding for the bronze iot events Publish test events to kafka topic Run Bronze Lakeflow Declarative Pipelines which will read from kafka source topic and write to: Events delta table into UC Create quarantine table as per data quality expectations Writes to external kafka topics Writes to external dbfs location as external delta sink Steps: Launch Command Prompt",
    "tags": [],
    "title": "Lakeflow Declarative Pipelines Sink Demo",
    "uri": "/demo/dlt_sink/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Demo",
    "content": "DAB Demo Overview This demo showcases how to use Databricks Asset Bundles (DABs) with DLT-Meta:\nThis demo will perform following steps:\nCreate dlt-meta schema’s for dataflowspec and bronze/silver layer Upload necessary resources to unity catalog volume Create DAB files with catalog, schema, file locations populated Deploy DAB to databricks workspace Run onboarding using DAB commands Run Bronze/Silver Pipelines using DAB commands Demo examples will showcase fan-out pattern in silver layer Demo example will show case custom transformations for bronze/silver layers Adding custom columns and metadata to Bronze tables Implementing SCD Type 1 to Silver tables Applying expectations to filter data in Silver tables Steps: Launch Command Prompt\nInstall Databricks CLI\nOnce you install Databricks CLI, authenticate your current machine to a Databricks Workspace: databricks auth login --host WORKSPACE_HOST Install Python package requirements:\n# Core requirements pip install \"PyYAML\u003e=6.0\" setuptools databricks-sdk # Development requirements pip install flake8==6.0 delta-spark==3.0.0 pytest\u003e=7.0.0 coverage\u003e=7.0.0 pyspark==3.5.5 Clone dlt-meta:\ngit clone https://github.com/databrickslabs/dlt-meta.git Navigate to project directory:\ncd dlt-meta Set python environment variable into terminal:\ndlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Generate DAB resources and set up schemas: This command will:\nGenerate DAB configuration files Create DLT-Meta schemas Upload necessary files to volumes python demo/generate_dabs_resources.py --source=cloudfiles --uc_catalog_name=\u003cyour_catalog_name\u003e --profile=\u003cyour_profile\u003e Note: If you don’t specify --profile, you’ll be prompted for your Databricks workspace URL and access token.\nDeploy and run the DAB bundle:\nNavigate to the DAB directory: cd demo/dabs Validate the bundle configuration: databricks bundle validate --profile=\u003cyour_profile\u003e Deploy the bundle to dev environment: databricks bundle deploy --target dev --profile=\u003cyour_profile\u003e Run the onboarding job: databricks bundle run onboard_people -t dev --profile=\u003cyour_profile\u003e Execute the pipelines: databricks bundle run execute_pipelines_people -t dev --profile=\u003cyour_profile\u003e",
    "description": "DAB Demo Overview This demo showcases how to use Databricks Asset Bundles (DABs) with DLT-Meta:\nThis demo will perform following steps:\nCreate dlt-meta schema’s for dataflowspec and bronze/silver layer Upload necessary resources to unity catalog volume Create DAB files with catalog, schema, file locations populated Deploy DAB to databricks workspace Run onboarding using DAB commands Run Bronze/Silver Pipelines using DAB commands Demo examples will showcase fan-out pattern in silver layer Demo example will show case custom transformations for bronze/silver layers Adding custom columns and metadata to Bronze tables Implementing SCD Type 1 to Silver tables Applying expectations to filter data in Silver tables Steps: Launch Command Prompt",
    "tags": [],
    "title": "DAB Demo",
    "uri": "/demo/dab/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "Integration Tests: To demonstrate DLT-META’s integration tests and CI/CD capabilities using the Databricks SDK, you can follow these steps. These steps involve setting up a testing environment, writing integration tests, and configuring a CI/CD pipeline to run these tests",
    "description": "Integration Tests: To demonstrate DLT-META’s integration tests and CI/CD capabilities using the Databricks SDK, you can follow these steps. These steps involve setting up a testing environment, writing integration tests, and configuring a CI/CD pipeline to run these tests",
    "tags": [],
    "title": "Additional",
    "uri": "/additionals/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Additional",
    "content": "Run Integration Tests Initial steps Prerequisite: Datatbricks CLI installed as given here git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk dlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run integration test against cloudfile or eventhub or kafka using below options: If databricks profile configured using CLI then pass --profile \u003cprofile-name\u003e to below command otherwise provide workspace url and token in command line\n2a. Run the command for cloudfiles python integration_tests/run_integration_tests.py --source=cloudfiles uc_catalog_name=\u003c\u003c\u003e\u003e\n2b. Run the command for eventhub python integration-tests/run_integration_tests.py --cloud_provider_name=azure --dbr_version=15.3.x-scala2.12 --source=eventhub --dbfs_path=dbfs:/tmp/DLT-META/ --eventhub_name=iot --eventhub_secrets_scope_name=eventhubs_creds --eventhub_namespace=int_test-standard --eventhub_port=9093 --eventhub_producer_accesskey_name=producer --eventhub_consumer_accesskey_name=consumer\nFor eventhub integration tests, the following are the prerequisites: Needs eventhub instance running Use Databricks CLI, Create databricks secrets scope for eventhub keys (databricks secrets create-scope eventhubs_creds) Use Databricks CLI, Create databricks secrets to store producer and consumer keys using the scope created in step Following are the mandatory arguments for running EventHubs integration test Provide your eventhub topic : –eventhub_name Provide eventhub namespace : –eventhub_namespace Provide eventhub port : –eventhub_port Provide databricks secret scope name : –eventhub_secrets_scope_name Provide eventhub producer access key name : –eventhub_producer_accesskey_name Provide eventhub access key name : –eventhub_consumer_accesskey_name 2c. Run the command for kafka python3 integration_tests/run_integration_tests.py --cloud_provider_name=aws --dbr_version=15.3.x-scala2.12 --source=kafka --dbfs_path=dbfs:/tmp/DLT-META/ --kafka_topic_name=dlt-meta-integration-test --kafka_broker=host:9092\nFor kafka integration tests, the following are the prerequisites: Needs kafka instance running Following are the mandatory arguments for running EventHubs integration test Provide your kafka topic name : –kafka_topic_name Provide kafka_broker : –kafka_broker Once finished integration output file will be copied locally to integration-test-output_\u003crun_id\u003e.txt\nOutput of a successful run should have the following in the file\n,0 0,Completed Bronze Lakeflow Declarative Pipeline. 1,Completed Silver Lakeflow Declarative Pipeline. 2,Validating Lakeflow Declarative Pipeline Bronze and Silver Table Counts... 3,Validating Counts for Table bronze_7d1d3ccc9e144a85b07c23110ea50133.transactions. 4,Expected: 10002 Actual: 10002. Passed! 5,Validating Counts for Table bronze_7d1d3ccc9e144a85b07c23110ea50133.transactions_quarantine. 6,Expected: 7 Actual: 7. Passed! 7,Validating Counts for Table bronze_7d1d3ccc9e144a85b07c23110ea50133.customers. 8,Expected: 98928 Actual: 98928. Passed! 9,Validating Counts for Table bronze_7d1d3ccc9e144a85b07c23110ea50133.customers_quarantine. 10,Expected: 1077 Actual: 1077. Passed! 11,Validating Counts for Table silver_7d1d3ccc9e144a85b07c23110ea50133.transactions. 12,Expected: 8759 Actual: 8759. Passed! 13,Validating Counts for Table silver_7d1d3ccc9e144a85b07c23110ea50133.customers. 14,Expected: 87256 Actual: 87256. Passed!",
    "description": "Run Integration Tests Initial steps Prerequisite: Datatbricks CLI installed as given here git clone https://github.com/databrickslabs/dlt-meta.git cd dlt-meta python -m venv .venv source .venv/bin/activate pip install databricks-sdk dlt_meta_home=$(pwd) export PYTHONPATH=$dlt_meta_home Run integration test against cloudfile or eventhub or kafka using below options: If databricks profile configured using CLI then pass --profile \u003cprofile-name\u003e to below command otherwise provide workspace url and token in command line",
    "tags": [],
    "title": "Integration Tests",
    "uri": "/additionals/integration_tests/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Frequently Asked Questions (FAQs)",
    "uri": "/faq/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Frequently Asked Questions (FAQs)",
    "content": "Q. How do I get started ?\nPlease refer to the Getting Started guide\nQ. How do I create metadata DLT-META ?\nDLT-META needs following metadata files:\nOnboarding File captures input/output metadata Data Quality Rules File captures data quality rules Silver transformation File captures processing logic as sql Q. What is DataflowSpecs?\nDLT-META translates input metadata into Delta table as DataflowSpecs\nQ. How many Lakeflow Declarative Pipelines will be launched using DLT-META?\nDLT-META uses data_flow_group to launch Lakeflow Declarative Pipelines, so all the tables belongs to same group will be executed under single Lakeflow Declarative pipeline.\nQ. Can we run onboarding for bronze layer only?\nYes! Please follow below steps:\nBronze Metadata preparation (example) Onboarding Job Option#1: DLT-META CLI Option#2: Manual Job Use below parameters { \"onboard_layer\": \"bronze\", \"database\": \"dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"import_author\": \"Ravi\", \"version\": \"v1\", \"uc_enabled\": \"True\", \"overwrite\": \"True\", \"env\": \"dev\" } option#3: Databircks Notebook onboarding_params_map = { \"database\": \"uc_name.dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"bronze_dataflowspec_table\": \"bronze_dataflowspec_table\", \"overwrite\": \"True\", \"env\": \"dev\", \"version\": \"v1\", \"import_author\": \"Ravi\" } from src.onboard_dataflowspec import OnboardDataflowspec OnboardDataflowspec(spark, onboarding_params_map, uc_enabled=True).onboard_bronze_dataflow_spec() Q. Can we run onboarding for silver layer only? Yes! Please follow below steps:\nBronze Metadata preparation (example) Onboarding Job Option#1: DLT-META CLI Option#2: Manual Job Use below parameters { \"onboard_layer\": \"silver\", \"database\": \"dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"import_author\": \"Ravi\", \"version\": \"v1\", \"uc_enabled\": \"True\", \"overwrite\": \"True\", \"env\": \"dev\" } option#3: Databircks Notebook onboarding_params_map = { \"database\": \"uc_name.dlt_demo\", \"onboarding_file_path\": \"dbfs:/dlt-meta/conf/onboarding.json\", \"silver_dataflowspec_table\": \"silver_dataflowspec_table\", \"overwrite\": \"True\", \"env\": \"dev\", \"version\": \"v1\", \"import_author\": \"Ravi\" } from src.onboard_dataflowspec import OnboardDataflowspec OnboardDataflowspec(spark, onboarding_params_map, uc_enabled=True).onboard_silver_dataflow_spec() Q. How to chain multiple silver tables after bronze table?\nExample: After customers_cdc bronze table, can I have customers silver table reading from customers_cdc and another customers_clean silver table reading from customers_cdc? If so, how do I define these in onboarding.json?\nYou can run onboarding for additional silver customer_clean table by having onboarding file and silver transformation with filter condition for fan out.\nRun onboarding for slilver layer in append mode(“overwrite”: “False”) so it will append to existing silver tables. When you launch Lakeflow Declarative Pipeline it will read silver onboarding and run Lakeflow Declarative Pipeline for bronze source and silver as target\nQ. How can I do type1 or type2 merge to target table?\nUsing Lakeflow Declarative Pipeline’s dlt.create_auto_cdc_flow we can do type1 or type2 merge. DLT-META have tag in onboarding file as bronze_cdc_apply_changes or silver_cdc_apply_changes which maps to Lakeflow Declarative Pipeline’s create_auto_cdc_flow API. \"silver_cdc_apply_changes\": { \"keys\":[ \"customer_id\" ], \"sequence_by\":\"dmsTimestamp,enqueueTimestamp,sequenceId\", \"scd_type\":\"2\", \"apply_as_deletes\":\"Op = 'D'\", \"except_column_list\":[ \"Op\", \"dmsTimestamp\", \"_rescued_data\" ] } Q. How can I write to same target table using different sources?\nUsing Lakeflow Declarative Pipeline’s dlt.append_flow API we can write to same target from different sources. DLT-META have tag in onboarding file as bronze_append_flows and silver_append_flows dlt.append_flow API is mapped to [ { \"name\":\"customer_bronze_flow1\", \"create_streaming_table\":false, \"source_format\":\"cloudFiles\", \"source_details\":{ \"source_path_dev\":\"tests/resources/data/customers\", \"source_schema_path\":\"tests/resources/schema/customer_schema.ddl\" }, \"reader_options\":{ \"cloudFiles.format\":\"json\", \"cloudFiles.inferColumnTypes\":\"true\", \"cloudFiles.rescuedDataColumn\":\"_rescued_data\" }, \"once\":true }, { \"name\":\"customer_bronze_flow2\", \"create_streaming_table\":false, \"source_format\":\"delta\", \"source_details\":{ \"source_database\":\"{uc_catalog_name}.{bronze_schema}\", \"source_table\":\"customers_delta\" }, \"reader_options\":{ }, \"once\":false } ] Q. How to add autloaders file metadata to bronze table?\nDLT-META have tag source_metadata in onboarding json under source_details\n\"source_metadata\":{ \"include_autoloader_metadata_column\":\"True\", \"autoloader_metadata_col_name\":\"source_metadata\", \"select_metadata_cols\":{ \"input_file_name\":\"_metadata.file_name\", \"input_file_path\":\"_metadata.file_path\" } } include_autoloader_metadata_column flag will add _metadata column to target bronze dataframe. autoloader_metadata_col_name if this provided then will be used to rename _metadata to this value otherwise default is source_metadata select_metadata_cols:{key:value} will be used to extract columns from _metadata. key is target dataframe column name and value is expression used to add column from _metadata column Q. After upgrading dlt-meta, why do Lakeflow Declarative Pipeline fail with the message “Materializing tables in custom schemas is not supported,” and how can this be fixed?\nThis failure happens because the pipeline was created using Legacy Publishing mode, which does not support saving tables with catalog or schema qualifiers (such as catalog.schema.table). As a result, using qualified table names leads to an error:\ncom.databricks.pipelines.common.errors.DLTAnalysisException: Materializing tables in custom schemas is not supported. Please remove the database qualifier from table 'catalog_name.schema_name.table_name'\nTo resolve this, migrate the pipeline to the default (Databricks Publishing Mode) by following Databricks’ guide: Migrate to the default publishing mode.",
    "description": "Q. How do I get started ?\nPlease refer to the Getting Started guide\nQ. How do I create metadata DLT-META ?\nDLT-META needs following metadata files:\nOnboarding File captures input/output metadata Data Quality Rules File captures data quality rules Silver transformation File captures processing logic as sql Q. What is DataflowSpecs?\nDLT-META translates input metadata into Delta table as DataflowSpecs\nQ. How many Lakeflow Declarative Pipelines will be launched using DLT-META?",
    "tags": [],
    "title": "Execution",
    "uri": "/faq/execution/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Frequently Asked Questions (FAQs)",
    "content": "Q. What is DLT-META ?\nDLT-META is a solution/framework using Databricks Lakeflow Declarative Pipelines which helps you automate bronze and silver layer pipelines using CI/CD.\nQ. What are the benefits of using DLT-META ?\nWith DLT-META customers needs to only maintain metadata like onboarding.json, data quality rules and silver transformations and framework will take care of execution. In case of any input/output or data quality rules or silver transformation logic changes there will be only metadata changes using onboarding interface and no need to re-deploy pipelines. If you have 100s or 1000s of tables then DLT-META speeds up overall turn around time to production as customers needs to just produce metadata Q. What different types of reader are supported using DLT-META ?\nDLT-META uses Databricks Auto Loader, DELTA, KAFKA, EVENTHUB to read from s3/adls/blog storage.\nQ. Can DLT-META support any other readers?\nDLT-META can support any spark streaming reader. You can override read_bronze() api inside DataflowPipeline.py to support any reader\nQ. Who should use this framework ?\nData Engineering teams, who wants to automate data migrations at scale using CI/CD.",
    "description": "Q. What is DLT-META ?\nDLT-META is a solution/framework using Databricks Lakeflow Declarative Pipelines which helps you automate bronze and silver layer pipelines using CI/CD.\nQ. What are the benefits of using DLT-META ?\nWith DLT-META customers needs to only maintain metadata like onboarding.json, data quality rules and silver transformations and framework will take care of execution. In case of any input/output or data quality rules or silver transformation logic changes there will be only metadata changes using onboarding interface and no need to re-deploy pipelines. If you have 100s or 1000s of tables then DLT-META speeds up overall turn around time to production as customers needs to just produce metadata Q. What different types of reader are supported using DLT-META ?",
    "tags": [],
    "title": "General",
    "uri": "/faq/general/index.html"
  },
  {
    "breadcrumb": "DLT-META \u003e Frequently Asked Questions (FAQs)",
    "content": "Initial Setup Q1. Do I need to run an initial setup before using the DLT-META App?\nYes. Before using the DLT-META App, you must click the Setup button to create the required dlt-meta environment. This initializes the app and enables you to onboard or manage Lakeflow Declarative Pipelines.\nFeatures and Capabilities Q2. What are the main features of the DLT-META App?\nThe DLT-META App provides several key capabilities:\nOnboard new Lakeflow Declarative Pipeline through an interactive interface Deploy and manage pipelines directly in the app Run demo flows to explore example pipelines and usage patterns Use the command-line interface (CLI) to automate operations Access and Permissions Q3. Who can access and use the DLT-META App?\nOnly authenticated Databricks workspace users with appropriate permissions can access and use the app:\nYou need CAN_USE permission to run the app You need CAN_MANAGE permission to administer it The app can be shared within your workspace or account Every user must log in with their Databricks account credentials Resource Access Q4. How does catalog and schema access work in the DLT-META App?\nBy default, the app uses a dedicated Service Principal (SP) for all data and resource access:\nThe Service Principal needs explicit permissions (USE CATALOG, USE SCHEMA, SELECT) on all Unity Catalog resources User abilities depend on the Service Principal’s access, regardless of URL Optional On-Behalf-Of (OBO) mode uses individual user permissions",
    "description": "Initial Setup Q1. Do I need to run an initial setup before using the DLT-META App?\nYes. Before using the DLT-META App, you must click the Setup button to create the required dlt-meta environment. This initializes the app and enables you to onboard or manage Lakeflow Declarative Pipelines.\nFeatures and Capabilities Q2. What are the main features of the DLT-META App?\nThe DLT-META App provides several key capabilities:\nOnboard new Lakeflow Declarative Pipeline through an interactive interface Deploy and manage pipelines directly in the app Run demo flows to explore example pipelines and usage patterns Use the command-line interface (CLI) to automate operations Access and Permissions Q3. Who can access and use the DLT-META App?",
    "tags": [],
    "title": "App",
    "uri": "/faq/app_faq/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "Contributing to DLT-META At present, external contributions are not accepted\nInternal Databricks employees are welcome to contribute but are currently limited to tests and supporting refactoring until sufficient tests are in place.",
    "description": "Contributing to DLT-META At present, external contributions are not accepted\nInternal Databricks employees are welcome to contribute but are currently limited to tests and supporting refactoring until sufficient tests are in place.",
    "tags": [],
    "title": "Contributing",
    "uri": "/contributing/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "v0.0.10 Enhancements Added apply_changes_from_snapshot support in silver layer PR Added UI using databricks lakehouse app for onboarding/deploy commands PR Added support for non-Delta as sinks(delta, kafka) PR Added quarantine support in silver layer for data quality rules PR Added support for table comments, column comments, and cluster_by PR Added catalog support for sourceDetails and targetDetails PR Added DBDemos for dlt-meta PR Added YAML support for onboarding PR Fixed issue cluster by not working with bronze append only table PR Fixed issue view name containing period when using DPM PR Fixed issue CLI onboarding overwrite option always set to True PR Fixed issue Silver Lakeflow Declarative Pipeline not creating based on passed database PR Fixed issue PyPI download stats display PR Fixed issue Silver Data Quality not working PR Fixed issue Removed DPM flag check inside dataflowpipeline PR Fixed issue Updated dlt-meta demos into Delta Live Tables Notebook github PR Fixed issue Adding multiple col support for auto_cdc api PR Fixed issue Added support for custom transformations for Kafka/Delta PR v0.0.9 Enhancements Added apply_changes_from_snapshot api support in bronze layer: PR Added dlt append_flow api support for silver layer: PR Added support for file metadata columns for autoloader: PR Added support for Bring your own custom transformation: Issue Added support to Unify PyPI releases with GitHub OIDC: PR Added demo for append_flow and file_metadata options: PR Added Demo for silver fanout architecture: PR Added hugo-theme-relearn themee: PR Added unit tests to showcase silver layer fanout examples: PR Added liquid cluster support: PR Added support for UC Volume + Serverless support for CLI, Integration tests and Demos: PR Added Chaining bronze/silver pipelines into single DLT: PR Updates Fixed issue for No such file or directory: ‘/demo’ :PR Fixed issue DLT-META CLI onboard command issue for Azure: databricks.sdk.errors.platform.ResourceAlreadyExists :PR Fixed issue Changed dbfs.create to mkdirs for CLI: PR Fixed issue DLT-META CLI should use pypi lib instead of whl : PR Fixed issue Onboarding with multiple partition columns errors out: PR v0.0.8 Enhancements Added dlt append_flow api support: PR Added dlt append_flow api support for silver layer: PR Added support for file metadata columns for autoloader: PR Added support for Bring your own custom transformation: Issue Added support to Unify PyPI releases with GitHub OIDC: PR Added demo for append_flow and file_metadata options: PR Added Demo for silver fanout architecture: PR Added documentation in docs site for new features: PR Added unit tests to showcase silver layer fanout examples: PR Updates Fixed issue for No such file or directory: ‘/demo’ :PR Fixed issue DLT-META CLI onboard command issue for Azure: databricks.sdk.errors.platform.ResourceAlreadyExists :PR Fixed issue Changed dbfs.create to mkdirs for CLI: PR Fixed issue DLT-META CLI should use pypi lib instead of whl : PR v0.0.7 Enhancements 1. Mismatched Keys: Update read_dlt_delta() with key “source_database” instead of “database” #33 2. Create dlt-meta cli documentation #45 Readme and docs to include above features v0.0.6 Enhancements 1. Migrate to create_streaming_table api from create_streaming_live_table #37 Updates Readme and docs to include above features Added Data Quality support for silver layer Updated existing demos to incorporate unity catalog and integration test framework integration tests framework which can be used to launch demos v0.0.5 New Features 1. Unity Catalog Support (#28) 2. Databricks Labs CLI Support (#28) Added two commands for DLT-META onboard: Captures all onboarding details from command line and launch onboarding job to your databricks workspace deploy: Captures all Lakeflow Declarative Pipeline details from command line and launch Lakeflow Declarative Pipeline to your databricks workspace Updates Readme and docs to include above features Updated existing demos to incorporate unity catalog and integration test framework integration tests framework which can be used to launch demos v0.0.4 Bug Fixes Introduced new source detail option for eventhub: eventhub.accessKeySecretName v0.0.3 Bug Fixes Infer datatypes from sequence_by to __START_AT, __END_AT for apply changes API Removed Git release tag from github actions v0.0.2 New Features Table properties support for bronze, quarantine and silver tables using create_streaming_live_table api call Support for track history column using apply_changes api Support for delta as source Validation for bronze/silver onboarding Bug Fixes Input schema parsing issue in onboarding Updates Readme and docs to include above features v.0.0.1 Release v.0.0.1",
    "description": "v0.0.10 Enhancements Added apply_changes_from_snapshot support in silver layer PR Added UI using databricks lakehouse app for onboarding/deploy commands PR Added support for non-Delta as sinks(delta, kafka) PR Added quarantine support in silver layer for data quality rules PR Added support for table comments, column comments, and cluster_by PR Added catalog support for sourceDetails and targetDetails PR Added DBDemos for dlt-meta PR Added YAML support for onboarding PR Fixed issue cluster by not working with bronze append only table PR Fixed issue view name containing period when using DPM PR Fixed issue CLI onboarding overwrite option always set to True PR Fixed issue Silver Lakeflow Declarative Pipeline not creating based on passed database PR Fixed issue PyPI download stats display PR Fixed issue Silver Data Quality not working PR Fixed issue Removed DPM flag check inside dataflowpipeline PR Fixed issue Updated dlt-meta demos into Delta Live Tables Notebook github PR Fixed issue Adding multiple col support for auto_cdc api PR Fixed issue Added support for custom transformations for Kafka/Delta PR v0.0.9 Enhancements Added apply_changes_from_snapshot api support in bronze layer: PR Added dlt append_flow api support for silver layer: PR Added support for file metadata columns for autoloader: PR Added support for Bring your own custom transformation: Issue Added support to Unify PyPI releases with GitHub OIDC: PR Added demo for append_flow and file_metadata options: PR Added Demo for silver fanout architecture: PR Added hugo-theme-relearn themee: PR Added unit tests to showcase silver layer fanout examples: PR Added liquid cluster support: PR Added support for UC Volume + Serverless support for CLI, Integration tests and Demos: PR Added Chaining bronze/silver pipelines into single DLT: PR Updates Fixed issue for No such file or directory: ‘/demo’ :PR Fixed issue DLT-META CLI onboard command issue for Azure: databricks.sdk.errors.platform.ResourceAlreadyExists :PR Fixed issue Changed dbfs.create to mkdirs for CLI: PR Fixed issue DLT-META CLI should use pypi lib instead of whl : PR Fixed issue Onboarding with multiple partition columns errors out: PR v0.0.8 Enhancements Added dlt append_flow api support: PR Added dlt append_flow api support for silver layer: PR Added support for file metadata columns for autoloader: PR Added support for Bring your own custom transformation: Issue Added support to Unify PyPI releases with GitHub OIDC: PR Added demo for append_flow and file_metadata options: PR Added Demo for silver fanout architecture: PR Added documentation in docs site for new features: PR Added unit tests to showcase silver layer fanout examples: PR Updates Fixed issue for No such file or directory: ‘/demo’ :PR Fixed issue DLT-META CLI onboard command issue for Azure: databricks.sdk.errors.platform.ResourceAlreadyExists :PR Fixed issue Changed dbfs.create to mkdirs for CLI: PR Fixed issue DLT-META CLI should use pypi lib instead of whl : PR v0.0.7 Enhancements 1. Mismatched Keys: Update read_dlt_delta() with key “source_database” instead of “database” #33 2. Create dlt-meta cli documentation #45 Readme and docs to include above features v0.0.6 Enhancements 1. Migrate to create_streaming_table api from create_streaming_live_table #37 Updates Readme and docs to include above features Added Data Quality support for silver layer Updated existing demos to incorporate unity catalog and integration test framework integration tests framework which can be used to launch demos v0.0.5 New Features 1. Unity Catalog Support (#28) 2. Databricks Labs CLI Support (#28) Added two commands for DLT-META onboard: Captures all onboarding details from command line and launch onboarding job to your databricks workspace deploy: Captures all Lakeflow Declarative Pipeline details from command line and launch Lakeflow Declarative Pipeline to your databricks workspace Updates Readme and docs to include above features Updated existing demos to incorporate unity catalog and integration test framework integration tests framework which can be used to launch demos v0.0.4 Bug Fixes Introduced new source detail option for eventhub: eventhub.accessKeySecretName v0.0.3 Bug Fixes Infer datatypes from sequence_by to __START_AT, __END_AT for apply changes API Removed Git release tag from github actions v0.0.2 New Features Table properties support for bronze, quarantine and silver tables using create_streaming_live_table api call Support for track history column using apply_changes api Support for delta as source Validation for bronze/silver onboarding Bug Fixes Input schema parsing issue in onboarding Updates Readme and docs to include above features v.0.0.1 Release v.0.0.1",
    "tags": [],
    "title": "Releases",
    "uri": "/releases/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Project Overview DLT-META is a metadata-driven framework designed to work with Lakeflow Declarative Pipelines. This framework enables the automation of bronze and silver data pipelines by leveraging metadata recorded in an onboarding JSON file. This file, known as the Dataflowspec, serves as the data flow specification, detailing the source and target metadata required for the pipelines.\nIn practice, a single generic pipeline reads the Dataflowspec and uses it to orchestrate and run the necessary data processing workloads. This approach streamlines the development and management of data pipelines, allowing for a more efficient and scalable data processing workflow\nLakeflow Declarative Pipelines and DLT-META are designed to complement each other. Lakeflow Declarative Pipelines provide a declarative, intent-driven foundation for building and managing data workflows, while DLT-META adds a powerful configuration-driven layer that automates and scales pipeline creation. By combining these approaches, teams can move beyond manual coding to achieve true enterprise-level agility, governance, and efficiency, templatizing and automating pipelines for any scale of modern data-driven business\nDLT-META components: Metadata Interface Capture input/output metadata in onboarding file Capture Data Quality Rules Capture processing logic as sql in Silver transformation file Generic Lakeflow Declarative pipeline Apply appropriate readers based on input metadata Apply data quality rules with Lakeflow Declarative Pipelines expectations Apply CDC apply changes if specified in metadata Builds Lakeflow Declarative Pipelines graph based on input/output metadata Launch Lakeflow Declarative Pipelines pipeline High-Level Solution overview: How does DLT-META work? Metadata Preparation\nOnboarding Job\nOption#1: DLT-META CLI Option#2: Manual Job option#3: Databricks Notebook Dataflow Lakeflow Declarative Pipeline\nOption#1: DLT-META CLI Option#2: DLT-META MANUAL DLT-META Lakeflow Declarative Pipelines Features support Features DLT-META Support Input data sources Autoloader, Delta, Eventhub, Kafka, snapshot Medallion architecture layers Bronze, Silver Custom transformations Bronze, Silver layer accepts custom functions Data Quality Expecations Support Bronze, Silver layer Quarantine table support Bronze layer create_auto_cdc_flow API support Bronze, Silver layer create_auto_cdc_from_snapshot_flow API support Bronze layer append_flow API support Bronze layer Liquid cluster support Bronze, Bronze Quarantine, Silver tables DLT-META CLI databricks labs dlt-meta onboard, databricks labs dlt-meta deploy Bronze and Silver pipeline chaining Deploy dlt-meta pipeline with layer=bronze_silver option using default publishing mode create_sink API support Supported formats:external delta table , kafka Bronze, Silver layers Databricks Asset Bundles Supported DLT-META UI Uses Databricks Lakehouse DLT-META App How much does it cost ? DLT-META does not have any direct cost associated with it other than the cost to run the Databricks Lakeflow Declarative Pipelines on your environment.The overall cost will be determined primarily by the [Databricks Lakeflow Declarative Pipelines Pricing] (https://www.databricks.com/product/pricing/lakeflow-declarative-pipelines)\nMore questions Refer to the FAQ\nGetting Started Refer to the Getting Started guide\nProject Support Please note that all projects in the databrickslabs github account are provided for your exploration only, and are not formally supported by Databricks with Service Level Agreements (SLAs). They are provided AS-IS and we do not make any guarantees of any kind. Please do not submit a support ticket relating to any issues arising from the use of these projects.\nAny issues discovered through the use of this project should be filed as GitHub Issues on the Repo. They will be reviewed as time permits, but there are no formal SLAs for support.\nContributing See our CONTRIBUTING for more details.",
    "description": "Project Overview DLT-META is a metadata-driven framework designed to work with Lakeflow Declarative Pipelines. This framework enables the automation of bronze and silver data pipelines by leveraging metadata recorded in an onboarding JSON file. This file, known as the Dataflowspec, serves as the data flow specification, detailing the source and target metadata required for the pipelines.\nIn practice, a single generic pipeline reads the Dataflowspec and uses it to orchestrate and run the necessary data processing workloads. This approach streamlines the development and management of data pipelines, allowing for a more efficient and scalable data processing workflow",
    "tags": [],
    "title": "DLT-META",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "DLT-META",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
